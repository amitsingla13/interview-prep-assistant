<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="description" content="AI-powered interview preparation and language practice assistant">
    <meta name="theme-color" content="#667eea">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="apple-mobile-web-app-title" content="Interview Prep">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <title>Interview Preparation Assistant</title>
    <!-- PWA -->
    <link rel="manifest" href="/static/manifest.json">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="apple-touch-icon" sizes="192x192" href="/static/icons/icon-192x192.png">
    <link rel="apple-touch-icon" sizes="512x512" href="/static/icons/icon-512x512.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
    <style>
        * { box-sizing: border-box; margin: 0; padding: 0; }
        html, body {
            height: 100%;
            overflow: hidden;
        }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            display: flex;
            justify-content: center;
            align-items: center;
        }
        .container {
            width: 100%;
            max-width: 600px;
            height: 100vh;
            max-height: 100vh;
            background: #fff;
            overflow: hidden;
            display: flex;
            flex-direction: column;
        }
        @media (min-height: 700px) and (min-width: 640px) {
            .container {
                height: 95vh;
                max-height: 95vh;
                border-radius: 20px;
                box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            }
        }
        @media (max-width: 639px) {
            .container {
                max-width: 100%;
                border-radius: 0;
            }
        }
        .header {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 15px 20px;
            text-align: center;
            flex-shrink: 0;
        }
        .header h1 { font-size: 20px; margin-bottom: 3px; }
        .header p { font-size: 12px; opacity: 0.85; }
        .mode-buttons {
            display: flex;
            gap: 8px;
            padding: 12px 15px;
            justify-content: center;
            flex-wrap: wrap;
            flex-shrink: 0;
        }
        .mode-btn {
            padding: 12px 20px;
            border: none;
            border-radius: 25px;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
            color: white;
        }
        .mode-btn.interview { background: #007bff; }
        .mode-btn.interview:hover { background: #0056b3; }
        .mode-btn.language { background: #28a745; }
        .mode-btn.language:hover { background: #1e7e34; }
        .mode-btn.helpdesk { background: #17a2b8; }
        .mode-btn.helpdesk:hover { background: #117a8b; }
        .mode-btn.reset { background: #dc3545; font-size: 12px; padding: 8px 15px; transition: all 0.3s; }
        .mode-btn.reset:hover { background: #c82333; }
        .mode-btn.reset.call-active { background: #28a745; }
        .mode-btn.reset.call-active:hover { background: #1e7e34; }
        /* Active mode highlight */
        .mode-btn.active {
            outline: 3px solid #FFD700;
            box-shadow: 0 0 0 4px rgba(255,215,0,0.5), 0 4px 15px rgba(0,0,0,0.3);
            transform: scale(1.1);
            filter: brightness(1.2);
            position: relative;
        }
        .mode-btn.active::after {
            content: '\f00c';
            font-family: 'Font Awesome 5 Free';
            font-weight: 900;
            position: absolute;
            top: -6px;
            right: -6px;
            background: #FFD700;
            color: #333;
            border-radius: 50%;
            width: 18px;
            height: 18px;
            font-size: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        #language-picker {
            display: none;
            padding: 0 20px 15px;
            text-align: center;
        }
        #language-picker select {
            padding: 10px 15px;
            border-radius: 10px;
            border: 2px solid #28a745;
            font-size: 14px;
            margin-right: 10px;
        }
        #language-picker button {
            padding: 10px 20px;
            background: #28a745;
            color: white;
            border: none;
            border-radius: 10px;
            font-size: 14px;
            cursor: pointer;
        }

        .chat-area {
            flex: 1;
            min-height: 0;
            overflow-y: auto;
            padding: 15px;
            background: #f8f9fa;
        }
        .message {
            margin-bottom: 15px;
            display: flex;
            align-items: flex-start;
            gap: 10px;
        }
        .message.bot { justify-content: flex-start; }
        .message.user { justify-content: flex-end; }
        .message .avatar {
            width: 36px; height: 36px;
            border-radius: 50%;
            display: flex; align-items: center; justify-content: center;
            font-size: 16px; color: white; flex-shrink: 0;
        }
        .message.bot .avatar { background: #667eea; }
        .message.user .avatar { background: #28a745; }
        .bubble {
            max-width: 75%;
            padding: 12px 16px;
            border-radius: 18px;
            font-size: 14px;
            line-height: 1.5;
        }
        .message.bot .bubble {
            background: white;
            color: #333;
            border-bottom-left-radius: 4px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }
        .message.user .bubble {
            background: #007bff;
            color: white;
            border-bottom-right-radius: 4px;
        }
        .message .audio-player {
            margin-top: 8px;
        }
        .message .audio-player audio {
            height: 32px;
            width: 100%;
        }

        .input-area {
            display: flex;
            padding: 10px 15px;
            gap: 8px;
            border-top: 1px solid #eee;
            background: white;
            align-items: center;
            flex-shrink: 0;
        }
        .input-area input {
            flex: 1;
            padding: 12px 16px;
            border: 2px solid #e0e0e0;
            border-radius: 25px;
            font-size: 14px;
            outline: none;
            transition: border-color 0.3s;
        }
        .input-area input:focus { border-color: #667eea; }
        .send-btn {
            width: 44px; height: 44px;
            border-radius: 50%;
            border: none;
            background: #007bff;
            color: white;
            font-size: 18px;
            cursor: pointer;
            transition: background 0.3s;
        }
        .send-btn:hover { background: #0056b3; }
        .mic-btn {
            width: 50px; height: 50px;
            border-radius: 50%;
            border: none;
            background: #28a745;
            color: white;
            font-size: 20px;
            cursor: pointer;
            transition: all 0.3s;
        }
        .mic-btn.recording {
            background: #28a745;
            animation: pulse 1s infinite;
            box-shadow: 0 0 20px rgba(40,167,69,0.5);
        }
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.15); }
            100% { transform: scale(1); }
        }
        .status-bar {
            text-align: center;
            padding: 5px;
            font-size: 11px;
            color: #666;
            background: #f0f0f0;
            flex-shrink: 0;
        }
        .typing-indicator {
            display: none;
            padding: 10px 20px;
            color: #999;
            font-size: 13px;
            font-style: italic;
        }
        /* Voice/Text toggle */
        .voice-toggle-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 6px 15px;
            flex-shrink: 0;
            background: #f0f0f0;
            border-top: 1px solid #e0e0e0;
        }
        .voice-toggle {
            display: flex;
            align-items: center;
            gap: 8px;
            font-size: 13px;
            color: #555;
        }
        .voice-toggle label {
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 6px;
        }
        .toggle-switch {
            position: relative;
            width: 42px;
            height: 22px;
            display: inline-block;
        }
        .toggle-switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .toggle-slider {
            position: absolute;
            cursor: pointer;
            top: 0; left: 0; right: 0; bottom: 0;
            background-color: #ccc;
            transition: 0.3s;
            border-radius: 22px;
        }
        .toggle-slider:before {
            position: absolute;
            content: "";
            height: 16px;
            width: 16px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: 0.3s;
            border-radius: 50%;
        }
        .toggle-switch input:checked + .toggle-slider {
            background-color: #28a745;
        }
        .toggle-switch input:checked + .toggle-slider:before {
            transform: translateX(20px);
        }
        .session-timer {
            font-size: 12px;
            color: #888;
            font-family: monospace;
        }
        /* Play button for on-demand TTS */
        .play-tts-btn {
            background: none;
            border: 1px solid #667eea;
            color: #667eea;
            border-radius: 15px;
            padding: 4px 10px;
            font-size: 12px;
            cursor: pointer;
            margin-top: 5px;
            transition: all 0.2s;
        }
        .play-tts-btn:hover {
            background: #667eea;
            color: white;
        }
        .play-tts-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        .play-tts-btn i {
            margin-right: 4px;
        }
        .cost-badge {
            font-size: 10px;
            padding: 2px 6px;
            border-radius: 8px;
            margin-left: 6px;
        }
        .cost-badge.saving { background: #d4edda; color: #155724; }
        .cost-badge.active { background: #fff3cd; color: #856404; }

        /* Interview Setup Panel */
        .interview-setup {
            display: none;
            padding: 12px 15px;
            background: #f0f4ff;
            border-bottom: 1px solid #dde;
            flex-shrink: 0;
        }
        .interview-setup h3 {
            font-size: 14px;
            color: #333;
            margin-bottom: 8px;
        }
        .upload-row {
            display: flex;
            gap: 8px;
            margin-bottom: 8px;
            align-items: center;
        }
        .upload-row label {
            font-size: 12px;
            color: #555;
            min-width: 90px;
            font-weight: 600;
        }
        .upload-row .file-input-wrapper {
            flex: 1;
            position: relative;
        }
        .upload-row input[type="file"] {
            font-size: 12px;
            width: 100%;
        }
        .upload-row .file-status {
            font-size: 11px;
            color: #28a745;
            margin-left: 5px;
        }
        .upload-row .file-remove {
            font-size: 11px;
            color: #dc3545;
            cursor: pointer;
            margin-left: 5px;
            text-decoration: underline;
        }
        .setup-start-btn {
            width: 100%;
            padding: 10px;
            border: none;
            border-radius: 10px;
            background: #007bff;
            color: white;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: background 0.3s;
        }
        .setup-start-btn:hover { background: #0056b3; }
        .setup-start-btn:disabled {
            background: #ccc;
            cursor: not-allowed;
        }
        .optional-tag {
            font-size: 10px;
            color: #999;
            font-weight: normal;
        }

        /* Video Avatar */
        .video-avatar-container {
            display: none;
            text-align: center;
            padding: 8px 15px;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            flex-shrink: 0;
            position: relative;
        }
        .avatar-visual {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            margin: 0 auto;
            position: relative;
            overflow: hidden;
            border: 3px solid #667eea;
            background: linear-gradient(135deg, #667eea, #764ba2);
            display: flex;
            align-items: center;
            justify-content: center;
        }
        .avatar-visual video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        .avatar-visual .avatar-fallback {
            font-size: 42px;
            color: white;
        }
        .avatar-visual .avatar-svg {
            width: 100%;
            height: 100%;
            border-radius: 50%;
            object-fit: cover;
        }
        .video-avatar-container .avatar-name {
            color: #ccc;
            font-size: 12px;
            margin-top: 4px;
        }
        .video-avatar-container .speaking-ring {
            position: absolute;
            top: 8px;
            left: 50%;
            transform: translateX(-50%);
            width: 110px;
            height: 110px;
            border-radius: 50%;
            border: 3px solid transparent;
            transition: border-color 0.3s;
            pointer-events: none;
        }
        .video-avatar-container .speaking-ring.active {
            border-color: #28a745;
            animation: avatar-pulse 1.5s infinite;
        }
        @keyframes avatar-pulse {
            0% { box-shadow: 0 0 0 0 rgba(40,167,69,0.4); }
            70% { box-shadow: 0 0 0 12px rgba(40,167,69,0); }
            100% { box-shadow: 0 0 0 0 rgba(40,167,69,0); }
        }

        /* Transcript Panel */
        .transcript-panel {
            display: none;
            max-height: 60px;
            overflow-y: auto;
            padding: 6px 15px;
            background: rgba(0,0,0,0.03);
            border-top: 1px solid #eee;
            flex-shrink: 0;
            font-size: 12px;
            color: #555;
        }
        .transcript-panel .transcript-line {
            margin-bottom: 3px;
            line-height: 1.4;
        }
        .transcript-panel .transcript-line.user-line {
            color: #007bff;
        }
        .transcript-panel .transcript-line.bot-line {
            color: #333;
        }
        .transcript-panel .transcript-label {
            font-weight: 600;
            font-size: 11px;
        }
    </style>
</head>
<body>
    <!-- ===== Persistent Top Navbar ===== -->
    <nav style="background:linear-gradient(135deg,#667eea,#764ba2);padding:10px 20px;display:flex;align-items:center;justify-content:space-between;box-shadow:0 2px 12px rgba(0,0,0,.15);position:sticky;top:0;z-index:9999;flex-shrink:0;">
        <a href="/dashboard" style="color:#fff;text-decoration:none;font-weight:700;font-size:1rem;font-family:'Segoe UI',Tahoma,sans-serif;"><i class="fas fa-arrow-left"></i> &nbsp;Dashboard</a>
        <div style="display:flex;align-items:center;gap:14px;">
            <span style="color:rgba(255,255,255,.9);font-size:.88rem;font-family:'Segoe UI',Tahoma,sans-serif;">{{ username }}</span>
            <a href="/logout" style="padding:6px 16px;background:rgba(255,255,255,.18);color:#fff;border:1.5px solid rgba(255,255,255,.35);border-radius:8px;text-decoration:none;font-weight:600;font-size:.84rem;font-family:'Segoe UI',Tahoma,sans-serif;transition:background .2s;"><i class="fas fa-sign-out-alt"></i> Log Out</a>
        </div>
    </nav>

    <div class="container">
        <div class="header">
            <h1><i class="fas fa-headset"></i> Interview Prep Assistant</h1>
            <p>Practice interviews, language speaking & IT support with AI</p>
        </div>

        <div class="mode-buttons" id="mode-buttons">
            <button class="mode-btn interview" id="btn-interview" onclick="startInterview()">
                <i class="fas fa-briefcase"></i> Interview
            </button>
            <button class="mode-btn language" id="btn-language" onclick="showLanguagePicker()">
                <i class="fas fa-language"></i> Language
            </button>
            <button class="mode-btn helpdesk" id="btn-helpdesk" onclick="startHelpdesk()">
                <i class="fas fa-headset"></i> Helpdesk
            </button>
            <button class="mode-btn reset" id="btn-reset" onclick="resetSession()">
                <i class="fas fa-phone" id="end-icon"></i> <span id="end-label">End</span>
            </button>
        </div>

        <div id="language-picker">
            <select id="lang-select">
                <option value="en">English</option>
                <option value="es">Spanish</option>
                <option value="fr">French</option>
                <option value="de">German</option>
                <option value="zh">Chinese</option>
                <option value="hi">Hindi</option>
                <option value="ja">Japanese</option>
                <option value="ko">Korean</option>
                <option value="pt">Portuguese</option>
                <option value="ar">Arabic</option>
                <option value="ru">Russian</option>
                <option value="it">Italian</option>
                <option value="nl">Dutch</option>
            </select>
            <button onclick="startLanguageTest()">Start</button>
        </div>

        <!-- Interview Setup Panel (CV + Job Profile upload) -->
        <div class="interview-setup" id="interview-setup">
            <h3><i class="fas fa-file-upload"></i> Prepare for Interview</h3>
            <div class="upload-row">
                <label>CV / Resume <span class="optional-tag">(optional)</span></label>
                <div class="file-input-wrapper">
                    <input type="file" id="cv-upload" accept=".pdf,.doc,.docx,.txt" onchange="handleFileUpload('cv')">
                    <span class="file-status" id="cv-status"></span>
                </div>
            </div>
            <div class="upload-row">
                <label>Job Profile <span class="optional-tag">(optional)</span></label>
                <div class="file-input-wrapper">
                    <input type="file" id="job-upload" accept=".pdf,.doc,.docx,.txt" onchange="handleFileUpload('job')">
                    <span class="file-status" id="job-status"></span>
                </div>
            </div>
            <button class="setup-start-btn" id="setup-start-btn" onclick="startInterviewWithContext()">
                <i class="fas fa-play"></i> Start Interview
            </button>
        </div>

        <!-- Video Avatar for Interviewer -->
        <div class="video-avatar-container" id="video-avatar">
            <div class="speaking-ring" id="speaking-ring"></div>
            <div class="avatar-visual" id="avatar-visual">
                <img id="avatar-svg" class="avatar-svg" src="" alt="Avatar" style="display:none;">
                <i class="fas fa-user-tie avatar-fallback" id="avatar-fallback"></i>
            </div>
            <div class="avatar-name" id="avatar-name">Charlotte â€” Interviewer</div>
        </div>

        <div class="chat-area" id="chat-area"></div>
        <!-- Live Transcript Panel -->
        <div class="transcript-panel" id="transcript-panel">
            <div id="transcript-content"></div>
        </div>

        <div class="typing-indicator" id="typing">
            <i class="fas fa-circle-notch fa-spin"></i> <span id="thinking-label">Interviewer is thinking...</span>
        </div>
        <div class="status-bar" id="status">Choose a mode to start</div>

        <div class="voice-toggle-bar">
            <div class="voice-toggle">
                <label>
                    <i class="fas fa-keyboard" id="mode-icon-text"></i>
                    <span class="toggle-switch">
                        <input type="checkbox" id="voice-toggle" onchange="toggleVoiceMode()" checked>
                        <span class="toggle-slider"></span>
                    </span>
                    <i class="fas fa-volume-up" id="mode-icon-voice"></i>
                    <span id="voice-mode-label">Voice mode</span>
                    <span class="cost-badge active" id="cost-badge">realtime AI</span>
                </label>
            </div>
            <div class="session-timer" id="session-timer">00:00</div>
        </div>

        <div class="input-area">
            <input type="text" id="text-input" placeholder="Type a message..." maxlength="2000" onkeypress="if(event.key==='Enter')sendText()">
            <button class="send-btn" onclick="sendText()" title="Send text">
                <i class="fas fa-paper-plane"></i>
            </button>
            <button class="mic-btn" id="mic-btn" onclick="toggleRecording()" title="Tap to start/stop voice recording">
                <i class="fas fa-microphone"></i>
            </button>
        </div>

        <div style="text-align:center; padding:6px; font-size:11px; color:#999; background:#fff; flex-shrink:0; border-top:1px solid #eee;" class="footer-links">
            <a href="/privacy" style="color:#888; text-decoration:none; margin:0 8px;">Privacy Policy</a>
            <span style="color:#ccc;">|</span>
            <a href="/terms" style="color:#888; text-decoration:none; margin:0 8px;">Terms of Service</a>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.7.4/socket.io.min.js"></script>
    <script>
        const socket = io({
            reconnection: true,
            reconnectionAttempts: 10,
            reconnectionDelay: 1000,
            reconnectionDelayMax: 5000,
            timeout: 20000
        });
        const chatArea = document.getElementById('chat-area');
        const textInput = document.getElementById('text-input');
        const micBtn = document.getElementById('mic-btn');
        const typingEl = document.getElementById('typing');
        const statusEl = document.getElementById('status');

        // ============================================================
        // CONNECTION: Reconnection & error handling
        // ============================================================
        let isConnected = false;

        socket.on('connect', () => {
            isConnected = true;
            statusEl.textContent = 'Connected â€” choose a mode to start';
            statusEl.style.color = '#666';
            // Sync voice mode default with server
            socket.emit('toggle_voice_mode', { voice_mode: voiceModeOn });
        });

        socket.on('disconnect', (reason) => {
            isConnected = false;
            typingEl.style.display = 'none';
            statusEl.textContent = 'Disconnected â€” reconnecting...';
            statusEl.style.color = '#dc3545';
            if (reason === 'io server disconnect') {
                socket.connect(); // Server disconnected us, reconnect manually
            }
        });

        socket.on('reconnect', (attemptNumber) => {
            isConnected = true;
            statusEl.textContent = 'Reconnected! You may need to restart your session.';
            statusEl.style.color = '#28a745';
            setTimeout(() => { statusEl.style.color = '#666'; }, 3000);
        });

        socket.on('reconnect_attempt', (attemptNumber) => {
            statusEl.textContent = `Reconnecting... (attempt ${attemptNumber})`;
        });

        socket.on('reconnect_failed', () => {
            statusEl.textContent = 'Connection lost. Please refresh the page.';
            statusEl.style.color = '#dc3545';
        });

        socket.on('connect_error', (err) => {
            statusEl.textContent = 'Connection error â€” retrying...';
            statusEl.style.color = '#dc3545';
        });

        let mediaRecorder = null;
        let audioChunks = [];
        let isRecording = false;
        let currentAudio = null;  // Track currently playing audio
        let botIsPlaying = false; // True while bot audio is playing
        let isProcessing = false; // True while waiting for bot response (prevents message piling)

        // ============================================================
        // STREAMING AUDIO: Progressive chunk queue for natural playback
        // Blueprint: Streaming TTS Layer â€” chunk-based streaming
        // ============================================================
        let audioChunkQueue = [];     // Queue of {audio_b64, chunk_index} waiting to play
        let isPlayingChunks = false;  // True while playing through the queue
        let streamingTextBuffer = ''; // Accumulates text chunks for display
        let streamingBubbleEl = null; // Reference to the bubble being streamed into
        let streamStartTime = null;   // For latency tracking

        function enqueueAudioChunk(audio_b64, chunkIndex) {
            audioChunkQueue.push({ audio_b64, chunkIndex });
            if (!isPlayingChunks) {
                playNextChunk();
            }
        }

        function playNextChunk() {
            if (audioChunkQueue.length === 0) {
                isPlayingChunks = false;
                botIsPlaying = false;
                // If interrupt monitor is running, let IT hand off the pre-warmed mic
                // (avoids race where both try to start listening and pre-warmed stream is wasted)
                if (autoListenEnabled && !isRecording && !interruptMonitorInterval) {
                    setTimeout(() => startListening(), 30); // Tiny echo-guard delay
                }
                return;
            }

            isPlayingChunks = true;
            botIsPlaying = true;
            const chunk = audioChunkQueue.shift();
            const audioBytes = Uint8Array.from(atob(chunk.audio_b64), c => c.charCodeAt(0));
            const blob = new Blob([audioBytes], { type: 'audio/ogg; codecs=opus' });
            const url = URL.createObjectURL(blob);

            currentAudio = new Audio(url);
            currentAudio.onended = () => {
                URL.revokeObjectURL(url);
                currentAudio = null;
                // Play next chunk in queue
                playNextChunk();
            };
            currentAudio.onerror = () => {
                URL.revokeObjectURL(url);
                currentAudio = null;
                playNextChunk();
            };
            currentAudio.play().catch(e => {
                console.log('Chunk autoplay blocked:', e);
                currentAudio = null;
                playNextChunk();
            });
        }

        function flushAudioQueue() {
            audioChunkQueue = [];
            isPlayingChunks = false;
        }

        // ============================================================
        // COST OPTIMIZATION: Voice mode state
        // ============================================================
        let voiceModeOn = true;    // Voice-first by default
        let sessionStartTime = Date.now();
        let sessionTimerInterval = null;
        let pendingTtsMessages = {};  // {msg_id: element} for on-demand TTS
        let currentMode = null;        // Track active mode: 'interview', 'helpdesk', 'language'
        let currentLanguage = 'en';   // Track selected language for language mode
        let waitingForBotAudio = false; // True between text_chunk and audio_chunk (pre-warm phase)

        // CV & Job Profile context (uploaded text)
        let uploadedCV = '';
        let uploadedJobProfile = '';

        // Role-specific labels
        const ROLE_LABELS = {
            interview: { thinking: 'Interviewer is thinking...', name: 'Charlotte â€” Interviewer', icon: 'fa-user-tie' },
            helpdesk: { thinking: 'IT Helpdesk is thinking...', name: 'Sam â€” IT Support', icon: 'fa-headset' },
            language: { thinking: 'Language Coach is thinking...', name: 'Language Coach', icon: 'fa-language' },
        };

        // 6 Avatar personas with distinct appearances and greetings
        const AVATAR_PERSONAS = {
            interview: [
                { name: 'Charlotte', title: 'VP Strategy & Operations', avatar: '/static/avatars/charlotte.svg', icon: 'fa-user-tie', voice: 'marin',
                  greetings: [
                    "Hello! I'm Charlotte, I head up strategy and operations here. Lovely to meet you â€” so tell me a bit about yourself, what's your background?",
                    "Hi there! Charlotte here, VP of strategy. So glad you could make it today â€” let's start with what brought you to this opportunity?",
                    "Good to meet you! I'm Charlotte, I run strategy and operations. Let's keep this relaxed â€” why don't you tell me what excites you about this role?",
                    "Welcome! I'm Charlotte from the leadership team. Really looking forward to our chat â€” shall we start with a quick walk through your career journey?"
                  ]},
                { name: 'James', title: 'Director of Engineering', avatar: '/static/avatars/james.svg', icon: 'fa-user-tie', voice: 'cedar',
                  greetings: [
                    "Good day! I'm James, Director of Engineering. I've been looking forward to this â€” could you start by telling me about your technical background?",
                    "Hello! James here, I lead our engineering division. Let's dive right in â€” what's the most interesting project you've worked on recently?",
                    "Pleased to meet you! I'm James from engineering. I like to keep these conversations practical â€” so tell me, what kind of challenges do you enjoy solving?",
                    "Welcome aboard for today's chat! I'm James, I oversee engineering. Why don't we begin with what drew you to this field in the first place?"
                  ]},
                { name: 'Priya', title: 'Senior Technical Lead', avatar: '/static/avatars/priya.svg', icon: 'fa-user-tie', voice: 'marin',
                  greetings: [
                    "Hi! I'm Priya, Senior Technical Lead. Wonderful to connect with you â€” what's your story? Tell me about your journey so far.",
                    "Hello there! Priya here, I lead our tech team. I always enjoy meeting new people â€” so what technologies are you most passionate about?",
                    "Welcome! I'm Priya from the technical leadership team. Let's make this a real conversation â€” what's been keeping you busy professionally?",
                    "Great to meet you! I'm Priya, Senior Tech Lead. I'd love to hear about your experience â€” shall we start with your current role?"
                  ]},
                { name: 'Marcus', title: 'Head of Product', avatar: '/static/avatars/marcus.svg', icon: 'fa-user-tie', voice: 'cedar',
                  greetings: [
                    "Hey, welcome! I'm Marcus, Head of Product here. Let's keep things casual â€” tell me about yourself and what you're looking for.",
                    "What's up! Marcus here, I run the product team. Really glad to chat with you today â€” so what gets you fired up about this opportunity?",
                    "Hi there! I'm Marcus from the product side. I like to keep interviews like a good conversation â€” so why don't you kick us off with your background?",
                    "Good to see you! Marcus here, Head of Product. I've read through your profile and I'm curious â€” what would you say is your superpower?"
                  ]},
                { name: 'Elena', title: 'CTO', avatar: '/static/avatars/elena.svg', icon: 'fa-user-tie', voice: 'marin',
                  greetings: [
                    "Hello! I'm Elena, CTO here. I love meeting talented people â€” tell me, what's your story?",
                    "Hi! Elena here, I'm the Chief Technology Officer. Let's skip the formalities â€” what are you most proud of in your career so far?",
                    "Welcome! I'm Elena from the exec team. I believe the best interviews feel like a great conversation â€” so what brings you here today?",
                    "So glad to meet you! I'm Elena, the CTO. Let's start simple â€” if you had to describe yourself in one sentence, what would it be?"
                  ]},
                { name: 'Sophia', title: 'VP Human Resources', avatar: '/static/avatars/sophia.svg', icon: 'fa-user-tie', voice: 'marin',
                  greetings: [
                    "Hello! I'm Sophia, VP of HR. It's lovely to meet you â€” let's start with what motivates you in your career.",
                    "Hi there! Sophia here from HR leadership. I'm all about finding the right fit â€” so tell me, what kind of team environment do you thrive in?",
                    "Welcome! I'm Sophia, I lead our people strategy. I'd love to understand what you're looking for â€” what's your ideal next step?",
                    "Great to meet you! I'm Sophia from the leadership team. Let's have a real conversation â€” tell me about what you're passionate about professionally."
                  ]}
            ],
            helpdesk: [
                { name: 'Sam', title: 'IT Support Specialist', avatar: '/static/avatars/david.svg', icon: 'fa-headset', voice: 'cedar',
                  greetings: [
                    "Hey there, I'm Sam from IT support! What seems to be the trouble today?",
                    "Hi! Sam here from the helpdesk. How can I help you out today?",
                    "Hello! I'm Sam, your IT support buddy. What's going on with your tech today?",
                    "Hey! Sam from IT here. Let me know what's happening and we'll get it sorted!"
                  ]},
                { name: 'Marcus', title: 'Senior Tech Support', avatar: '/static/avatars/marcus.svg', icon: 'fa-headset', voice: 'cedar',
                  greetings: [
                    "Hi! Marcus from tech support here. Walk me through what's happening.",
                    "Hey there! I'm Marcus, senior support engineer. What can I do for you?",
                    "Hello! Marcus here. I'm ready to troubleshoot â€” tell me what's going on.",
                    "What's up! I'm Marcus from the IT team. Let's figure this out together!"
                  ]},
                { name: 'Priya', title: 'IT Helpdesk Lead', avatar: '/static/avatars/priya.svg', icon: 'fa-headset', voice: 'marin',
                  greetings: [
                    "Hi there! I'm Priya from IT support. How can I assist you today?",
                    "Hello! Priya here, helpdesk lead. Tell me what's going on and we'll fix it.",
                    "Hey! I'm Priya from the IT team. What issue are you running into?",
                    "Welcome to IT support! I'm Priya. Let me know what you need help with."
                  ]}
            ],
            language: [
                { name: 'Elena', title: 'Language Coach', avatar: '/static/avatars/elena.svg', icon: 'fa-language', voice: 'marin',
                  greetings: [] },
                { name: 'Charlotte', title: 'Conversation Partner', avatar: '/static/avatars/charlotte.svg', icon: 'fa-language', voice: 'marin',
                  greetings: [] },
                { name: 'Sophia', title: 'Language Tutor', avatar: '/static/avatars/sophia.svg', icon: 'fa-language', voice: 'marin',
                  greetings: [] }
            ]
        };

        // Currently selected persona (set when mode starts)
        let currentPersona = null;

        function pickRandomPersona(mode) {
            const personas = AVATAR_PERSONAS[mode] || AVATAR_PERSONAS.interview;
            const idx = Math.floor(Math.random() * personas.length);
            currentPersona = personas[idx];
            // Update ROLE_LABELS dynamically
            if (mode === 'interview') {
                ROLE_LABELS.interview.name = currentPersona.name + ' â€” ' + currentPersona.title;
            } else if (mode === 'helpdesk') {
                ROLE_LABELS.helpdesk.name = currentPersona.name + ' â€” ' + currentPersona.title;
            } else if (mode === 'language') {
                ROLE_LABELS.language.name = currentPersona.name + ' â€” ' + currentPersona.title;
            }
            return currentPersona;
        }

        // SECURITY: Escape HTML to prevent XSS
        function escapeHTML(str) {
            const div = document.createElement('div');
            div.appendChild(document.createTextNode(str));
            return div.innerHTML;
        }

        // --- Session Timer ---
        function startSessionTimer() {
            sessionStartTime = Date.now();
            if (sessionTimerInterval) clearInterval(sessionTimerInterval);
            sessionTimerInterval = setInterval(() => {
                const elapsed = Math.floor((Date.now() - sessionStartTime) / 1000);
                const mins = String(Math.floor(elapsed / 60)).padStart(2, '0');
                const secs = String(elapsed % 60).padStart(2, '0');
                document.getElementById('session-timer').textContent = `${mins}:${secs}`;
            }, 1000);
        }

        // --- Voice Mode Toggle ---
        function toggleVoiceMode() {
            const toggle = document.getElementById('voice-toggle');
            voiceModeOn = toggle.checked;
            socket.emit('toggle_voice_mode', { voice_mode: voiceModeOn });

            const label = document.getElementById('voice-mode-label');
            const badge = document.getElementById('cost-badge');
            if (voiceModeOn) {
                label.textContent = 'Voice mode';
                badge.textContent = 'realtime AI';
                badge.className = 'cost-badge active';
                // If a mode is active, connect Realtime API
                if (currentMode) {
                    connectRealtime(currentMode, currentLanguage, uploadedCV, uploadedJobProfile);
                }
            } else {
                label.textContent = 'Text mode';
                badge.textContent = '$$$ saving';
                badge.className = 'cost-badge saving';
                // Disconnect Realtime session
                disconnectRealtime();
            }
        }

        // ============================================================
        // OPENAI REALTIME API: WebRTC for ChatGPT-level voice quality
        // Audio goes directly browser â†” OpenAI (~300ms latency)
        // Server VAD, auto-interruption, natural turn-taking built in
        // ============================================================
        let realtimePc = null;          // RTCPeerConnection
        let realtimeDc = null;          // DataChannel for events
        let realtimeAudio = null;       // Audio element for model output
        let realtimeMicStream = null;   // Mic stream for WebRTC
        let realtimeConnected = false;  // True when data channel is open
        let realtimeSessionId = null;
        let realtimeTextBuffer = '';    // Accumulates model text response
        let realtimeBubbleEl = null;    // Current streaming bot bubble
        let realtimeMuted = false;      // Mic mute state (user-initiated)
        let realtimeModelSpeaking = false; // True while model is outputting audio

        async function connectRealtime(mode, language, cvText, jobText) {
            // Disconnect any existing session
            disconnectRealtime();

            // Kill ALL legacy voice components â€” they conflict with Realtime API
            stopBrowserSTT();
            stopInterruptionMonitor();
            stopVAD();
            if (isRecording && mediaRecorder) {
                try { mediaRecorder.stop(); } catch(e) {}
                isRecording = false;
            }
            releaseMicStream();
            stopCurrentAudio();
            flushAudioQueue();

            try {
                statusEl.textContent = 'ðŸ”„ Connecting to voice AI...';

                // 1. Get ephemeral token from our server
                // All config (instructions, voice, VAD) is baked into the token
                // POST with CV/Job context if available
                const tokenBody = {
                    mode: mode,
                    language: language || 'en',
                };
                if (cvText) tokenBody.cv_text = cvText;
                if (jobText) tokenBody.job_profile_text = jobText;
                // Pass persona info so backend can customize the prompt
                if (currentPersona) {
                    tokenBody.persona_name = currentPersona.name;
                    tokenBody.persona_title = currentPersona.title;
                    if (currentPersona.greetings && currentPersona.greetings.length > 0) {
                        const gIdx = Math.floor(Math.random() * currentPersona.greetings.length);
                        tokenBody.persona_greeting = currentPersona.greetings[gIdx];
                    }
                }

                const tokenRes = await fetch('/api/realtime/token', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(tokenBody),
                });
                if (!tokenRes.ok) {
                    const errText = await tokenRes.text();
                    throw new Error(`Token failed: ${tokenRes.status} ${errText}`);
                }
                const tokenData = await tokenRes.json();
                const EPHEMERAL_KEY = tokenData.client_secret?.value || tokenData.value;
                if (!EPHEMERAL_KEY) {
                    console.error('[Realtime] Token response:', tokenData);
                    throw new Error('No ephemeral key received');
                }
                console.log('[Realtime] Got ephemeral token');

                // 2. Create RTCPeerConnection
                realtimePc = new RTCPeerConnection();

                // 3. Set up audio playback for model responses
                realtimeAudio = document.createElement('audio');
                realtimeAudio.autoplay = true;
                realtimePc.ontrack = (e) => (realtimeAudio.srcObject = e.streams[0]);

                // 4. Add microphone audio track
                // Use simple constraints like OpenAI's reference implementation
                realtimeMicStream = await navigator.mediaDevices.getUserMedia({
                    audio: true,
                });
                const micTrack = realtimeMicStream.getTracks()[0];
                console.log('[Realtime] Mic track:', micTrack.label, 'enabled:', micTrack.enabled, 'muted:', micTrack.muted);
                realtimePc.addTrack(micTrack);

                // 5. Create data channel for Realtime events
                realtimeDc = realtimePc.createDataChannel('oai-events');
                realtimeDc.onopen = () => {
                    console.log('[Realtime] Data channel open â€” session active');
                    realtimeConnected = true;
                    micBtn.classList.add('recording');
                    statusEl.textContent = 'ðŸŽ¤ Connected â€” speak naturally!';

                    // Log connection details for debugging
                    if (realtimePc) {
                        const senders = realtimePc.getSenders();
                        console.log('[Realtime] Senders:', senders.length);
                        senders.forEach((s, i) => {
                            console.log(`[Realtime] Sender ${i}: kind=${s.track?.kind} enabled=${s.track?.enabled} readyState=${s.track?.readyState}`);
                        });
                        const receivers = realtimePc.getReceivers();
                        console.log('[Realtime] Receivers:', receivers.length);
                        receivers.forEach((r, i) => {
                            console.log(`[Realtime] Receiver ${i}: kind=${r.track?.kind}`);
                        });
                    }

                    // Trigger initial greeting from the AI
                    realtimeDc.send(JSON.stringify({
                        type: 'response.create',
                    }));
                };
                realtimeDc.onclose = () => {
                    console.log('[Realtime] Data channel closed');
                    realtimeConnected = false;
                    micBtn.classList.remove('recording');
                    statusEl.textContent = 'Voice session ended';
                };
                realtimeDc.onmessage = (event) => {
                    handleRealtimeEvent(JSON.parse(event.data));
                };

                // 6. Handle ICE connection state
                realtimePc.oniceconnectionstatechange = () => {
                    const state = realtimePc.iceConnectionState;
                    console.log('[Realtime] ICE state:', state);
                    if (state === 'failed' || state === 'disconnected') {
                        statusEl.textContent = 'Voice connection lost â€” reconnecting...';
                        setTimeout(() => {
                            if (!realtimeConnected && currentMode) {
                                connectRealtime(currentMode, currentLanguage, uploadedCV, uploadedJobProfile);
                            }
                        }, 2000);
                    }
                };

                // 7. Create SDP offer
                const offer = await realtimePc.createOffer();
                await realtimePc.setLocalDescription(offer);

                // 8. Connect DIRECTLY to OpenAI â€” matches reference implementation
                // Include ?model= even with ephemeral tokens (reference does this)
                const model = 'gpt-realtime';
                const sdpRes = await fetch(
                    `https://api.openai.com/v1/realtime/calls?model=${model}`,
                    {
                        method: 'POST',
                        body: offer.sdp,
                        headers: {
                            'Authorization': `Bearer ${EPHEMERAL_KEY}`,
                            'Content-Type': 'application/sdp',
                        },
                    }
                );

                if (!sdpRes.ok) {
                    const errText = await sdpRes.text();
                    console.error('[Realtime] SDP response:', sdpRes.status, errText);
                    throw new Error(`SDP exchange failed: ${sdpRes.status} ${errText}`);
                }

                const answerSdp = await sdpRes.text();
                console.log('[Realtime] Got SDP answer, length:', answerSdp.length);
                await realtimePc.setRemoteDescription({
                    type: 'answer',
                    sdp: answerSdp,
                });

                console.log('[Realtime] Connected via ephemeral token (direct to OpenAI)');

            } catch (err) {
                console.error('[Realtime] Connection error:', err);
                statusEl.textContent = 'Voice connection failed â€” using fallback mode';
                disconnectRealtime();
                // Fall back to old Socket.IO pipeline
                fallbackToLegacyVoice(mode, language);
            }
        }

        function disconnectRealtime() {
            if (realtimeDc) {
                try { realtimeDc.close(); } catch(e) {}
                realtimeDc = null;
            }
            if (realtimePc) {
                realtimePc.getSenders().forEach(sender => {
                    if (sender.track) sender.track.stop();
                });
                try { realtimePc.close(); } catch(e) {}
                realtimePc = null;
            }
            if (realtimeMicStream) {
                realtimeMicStream.getTracks().forEach(t => t.stop());
                realtimeMicStream = null;
            }
            if (realtimeAudio) {
                realtimeAudio.pause();
                realtimeAudio.srcObject = null;
                realtimeAudio = null;
            }
            realtimeConnected = false;
            realtimeSessionId = null;
            realtimeTextBuffer = '';
            realtimeBubbleEl = null;
            realtimeMuted = false;
            realtimeModelSpeaking = false;
            micBtn.classList.remove('recording');
        }

        function fallbackToLegacyVoice(mode, language) {
            // Use the old Socket.IO streaming pipeline as fallback
            socket.emit('toggle_voice_mode', { voice_mode: true });
            if (mode === 'interview') {
                showThinking();
                socket.emit('start_interview');
            } else if (mode === 'helpdesk') {
                showThinking();
                socket.emit('start_helpdesk');
            } else if (mode === 'language') {
                showThinking();
                socket.emit('start_language_test', { language: language });
            }
        }

        // --- Interrupt support: cancel in-flight response ---
        function interruptModel() {
            if (!realtimeConnected || !realtimeModelSpeaking) return;
            realtimeModelSpeaking = false;
            if (realtimeDc && realtimeDc.readyState === 'open') {
                // For WebRTC: use output_audio_buffer.clear to stop playback
                // and truncate conversation (recommended by OpenAI docs)
                realtimeDc.send(JSON.stringify({ type: 'response.cancel' }));
                realtimeDc.send(JSON.stringify({ type: 'output_audio_buffer.clear' }));
            }
            statusEl.textContent = 'ðŸŽ¤ Go ahead...';
            console.log('[Realtime] User interrupted model');
        }

        // Keyboard interrupt: Space or Enter while model is speaking
        document.addEventListener('keydown', (e) => {
            if (!realtimeConnected || !realtimeModelSpeaking) return;
            if (e.code === 'Space' || e.code === 'Enter') {
                // Don't trigger if typing in the text input
                if (e.target.tagName === 'INPUT' || e.target.tagName === 'TEXTAREA') return;
                e.preventDefault();
                interruptModel();
            }
        });

        function toggleRealtimeMute() {
            if (!realtimeMicStream) return;
            // If model is speaking, treat click as interrupt
            if (realtimeModelSpeaking) {
                interruptModel();
                return;
            }
            realtimeMuted = !realtimeMuted;
            realtimeMicStream.getTracks().forEach(track => {
                track.enabled = !realtimeMuted;
            });
            if (realtimeMuted) {
                micBtn.classList.remove('recording');
                statusEl.textContent = 'ðŸ”‡ Muted â€” click mic to unmute';
            } else {
                micBtn.classList.add('recording');
                statusEl.textContent = 'ðŸŽ¤ Listening â€” speak naturally';
            }
        }

        function handleRealtimeEvent(event) {
            switch (event.type) {
                case 'session.created':
                    realtimeSessionId = event.session?.id || event.session;
                    console.log('[Realtime] Session created:', realtimeSessionId);
                    const sess = event.session || {};
                    console.log('[Realtime] Model:', sess.model);
                    console.log('[Realtime] Voice:', JSON.stringify(sess.audio?.output));
                    console.log('[Realtime] VAD:', JSON.stringify(sess.audio?.input?.turn_detection));
                    console.log('[Realtime] Transcription:', JSON.stringify(sess.audio?.input?.transcription));
                    console.log('[Realtime] Modalities:', sess.output_modalities);
                    console.log('[Realtime] Instructions (first 100):', (sess.instructions || '').substring(0, 100));
                    break;

                case 'session.updated':
                    console.log('[Realtime] Session updated â€” semantic_vad active');
                    break;

                case 'input_audio_buffer.speech_started':
                    // User started speaking â€” semantic_vad detected meaningful speech
                    console.log('[Realtime] >>> Speech STARTED â€” mic audio IS flowing to OpenAI');
                    statusEl.textContent = 'ðŸŽ¤ Listening...';
                    typingEl.style.display = 'none';
                    setAvatarSpeaking(false);
                    // With interrupt_response: true, the server automatically
                    // cancels any in-progress model response. We just update UI.
                    if (realtimeModelSpeaking) {
                        realtimeModelSpeaking = false;
                        // Finalize partial bubble
                        if (realtimeBubbleEl && realtimeTextBuffer.trim()) {
                            realtimeBubbleEl.textContent = realtimeTextBuffer + ' [interrupted]';
                            realtimeBubbleEl.classList.remove('streaming-bubble');
                        }
                        realtimeTextBuffer = '';
                        realtimeBubbleEl = null;
                        console.log('[Realtime] Cancelled stale response â€” user is speaking');
                    }
                    break;

                case 'input_audio_buffer.speech_stopped':
                    console.log('[Realtime] >>> Speech STOPPED â€” processing user input');
                    statusEl.textContent = 'âš¡ Processing...';
                    typingEl.style.display = 'block';
                    break;

                case 'conversation.item.input_audio_transcription.completed':
                    // User's speech transcribed â€” show in chat
                    console.log('[Realtime] >>> User transcript:', event.transcript);
                    if (event.transcript && event.transcript.trim()) {
                        addUserMessage(event.transcript.trim());
                        addTranscriptLine('user', event.transcript.trim());
                        // Log to backend for conversation history
                        socket.emit('realtime_log', {
                            role: 'user',
                            text: event.transcript.trim(),
                            mode: currentMode,
                        });
                    }
                    break;

                case 'response.audio_transcript.delta':
                    // Model's text response streaming in real-time
                    typingEl.style.display = 'none';
                    const delta = event.delta;
                    realtimeTextBuffer += delta;

                    // Track model speaking state
                    if (!realtimeModelSpeaking) {
                        realtimeModelSpeaking = true;
                        statusEl.textContent = 'ðŸ”Š Speaking... (just speak to interrupt)';
                        setAvatarSpeaking(true);
                    }

                    if (!realtimeBubbleEl) {
                        const div = document.createElement('div');
                        div.className = 'message bot';
                        const icon = getBotIcon();
                        div.innerHTML = `
                            <div class="avatar"><i class="fas ${icon}"></i></div>
                            <div>
                                <div class="bubble streaming-bubble">${escapeHTML(realtimeTextBuffer)}</div>
                            </div>`;
                        chatArea.appendChild(div);
                        realtimeBubbleEl = div.querySelector('.streaming-bubble');
                    } else {
                        realtimeBubbleEl.textContent = realtimeTextBuffer;
                    }
                    chatArea.scrollTop = chatArea.scrollHeight;
                    break;

                case 'response.audio_transcript.done':
                    // Model finished this transcript segment
                    if (realtimeBubbleEl) {
                        realtimeBubbleEl.classList.remove('streaming-bubble');
                    }
                    // Add to transcript panel
                    if (realtimeTextBuffer.trim()) {
                        addTranscriptLine('bot', realtimeTextBuffer.trim());
                    }
                    // Log to backend
                    if (realtimeTextBuffer.trim()) {
                        socket.emit('realtime_log', {
                            role: 'assistant',
                            text: realtimeTextBuffer.trim(),
                            mode: currentMode,
                        });
                    }
                    realtimeTextBuffer = '';
                    realtimeBubbleEl = null;
                    break;

                case 'response.done':
                    typingEl.style.display = 'none';
                    realtimeModelSpeaking = false;
                    setAvatarSpeaking(false);
                    statusEl.textContent = 'ðŸŽ¤ Ready â€” speak anytime';
                    break;

                case 'response.cancelled':
                    // User interrupted â€” OpenAI auto-cancelled the response
                    typingEl.style.display = 'none';
                    statusEl.textContent = 'ðŸŽ¤ Go ahead...';
                    // Finalize any partial bubble
                    if (realtimeBubbleEl) {
                        if (realtimeTextBuffer.trim()) {
                            realtimeBubbleEl.textContent = realtimeTextBuffer + ' [interrupted]';
                            realtimeBubbleEl.classList.remove('streaming-bubble');
                            socket.emit('realtime_log', {
                                role: 'assistant',
                                text: realtimeTextBuffer.trim() + ' [interrupted]',
                                mode: currentMode,
                            });
                        } else {
                            realtimeBubbleEl.parentElement.parentElement.remove();
                        }
                    }
                    realtimeTextBuffer = '';
                    realtimeBubbleEl = null;
                    realtimeModelSpeaking = false;
                    break;

                case 'error':
                    console.error('[Realtime] Error:', event.error);
                    if (event.error && event.error.message) {
                        statusEl.textContent = 'âš ï¸ ' + event.error.message;
                    } else {
                        statusEl.textContent = 'âš ï¸ Voice error â€” try speaking again';
                    }
                    break;

                case 'rate_limits.updated':
                    // Info only â€” log for monitoring
                    console.log('[Realtime] Rate limits:', event.rate_limits);
                    break;

                default:
                    // Log all events for debugging (except high-frequency audio)
                    if (event.type && !event.type.startsWith('response.audio.delta')) {
                        console.log('[Realtime] Event:', event.type, JSON.stringify(event).substring(0, 200));
                    }
            }
        }

        // ============================================================
        // BROWSER SPEECH RECOGNITION: Real-time STT (instant, no upload)
        // Falls back to MediaRecorder+Whisper for non-English or unsupported browsers
        // ============================================================
        let speechRecognition = null;
        let useBrowserSTT = false;
        let browserSTTResult = '';
        let browserSTTFinal = false;

        // Check if browser SpeechRecognition is available
        const SpeechRecognitionAPI = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (SpeechRecognitionAPI) {
            useBrowserSTT = true;
            console.log('[STT] Browser SpeechRecognition available â€” using for instant transcription');
        } else {
            console.log('[STT] Browser SpeechRecognition not available â€” using Whisper fallback');
        }

        function startBrowserSTT() {
            // Don't run legacy STT when Realtime API handles transcription
            if (realtimeConnected) return false;
            if (!SpeechRecognitionAPI) return false;
            // Only use browser STT for English (other languages use Whisper for accuracy)
            if (currentMode === 'language') return false;

            try {
                speechRecognition = new SpeechRecognitionAPI();
                speechRecognition.continuous = true;
                speechRecognition.interimResults = true;
                speechRecognition.lang = 'en-US';
                speechRecognition.maxAlternatives = 1;
                browserSTTResult = '';
                browserSTTFinal = false;

                speechRecognition.onresult = (event) => {
                    let interim = '';
                    let finalTranscript = '';
                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        const transcript = event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            finalTranscript += transcript;
                        } else {
                            interim += transcript;
                        }
                    }
                    if (finalTranscript) {
                        browserSTTResult += (browserSTTResult ? ' ' : '') + finalTranscript.trim();
                    }
                    // Show live transcription in status
                    const display = browserSTTResult + (interim ? ' ' + interim : '');
                    if (display.trim()) {
                        statusEl.textContent = 'ðŸŽ¤ ' + display.trim().substring(0, 80) + (display.length > 80 ? '...' : '');
                    }
                };

                speechRecognition.onerror = (event) => {
                    if (event.error !== 'no-speech' && event.error !== 'aborted') {
                        console.log('[STT] Speech recognition error:', event.error);
                    }
                };

                speechRecognition.onend = () => {
                    // Browser auto-stops recognition after silence timeout
                    // Restart if we're still actively recording to keep capturing
                    if (isRecording && !botIsPlaying) {
                        try {
                            speechRecognition.start();
                            console.log('[STT] Restarted browser recognition (auto-ended)');
                        } catch(e) {
                            console.log('[STT] Could not restart:', e.message);
                        }
                    }
                };

                speechRecognition.start();
                return true;
            } catch (e) {
                console.log('[STT] Failed to start browser speech recognition:', e);
                return false;
            }
        }

        function stopBrowserSTT() {
            if (speechRecognition) {
                try { speechRecognition.stop(); } catch(e) {}
                speechRecognition = null;
            }
        }

        function getBrowserSTTText() {
            const text = browserSTTResult.trim();
            browserSTTResult = '';
            return text;
        }

        // --- VAD (Voice Activity Detection) ---
        let audioContext = null;
        let analyser = null;
        let micStream = null;
        let vadInterval = null;
        let silenceStart = null;
        let speechDetected = false;
        let autoListenEnabled = true;   // auto-listen enabled by default
        let wasInterrupted = false; // Track if user interrupted bot
        const SILENCE_THRESHOLD = 12;   // RMS below this = silence (sensitive for reliable detection)
        const SILENCE_DURATION = 1500;  // 1.5s silence â€” natural pacing (1s was cutting people off mid-thought)
        const VAD_CHECK_INTERVAL = 40;  // ms between VAD checks
        const INTERRUPT_THRESHOLD = 16; // RMS threshold for interrupt detection during bot playback
        const INTERRUPT_SPEECH_MIN_MS = 150; // 150ms to count as real interruption (near-instant)
        let speechStartTime = null;     // when current speech burst started
        let interruptionDetectedAt = null; // timestamp when interruption was detected

        // --- Lightweight interruption monitor (no recording, just volume check) ---
        let interruptMonitorStream = null;
        let interruptMonitorCtx = null;
        let interruptMonitorAnalyser = null;
        let interruptMonitorInterval = null;
        let interruptSpeechStart = null;

        // Pre-warm: acquire mic stream during bot playback so it's ready for instant handoff
        let prewarmedMicStream = null;

        async function startInterruptionMonitor() {
            // Don't run legacy interruption monitor when Realtime API handles everything
            if (realtimeConnected) return;
            stopInterruptionMonitor();
            try {
                // Acquire mic stream (will be handed off to startListening on interrupt)
                interruptMonitorStream = await navigator.mediaDevices.getUserMedia({
                    audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: true }
                });
                prewarmedMicStream = interruptMonitorStream; // Save for handoff
                interruptMonitorCtx = new (window.AudioContext || window.webkitAudioContext)();
                const source = interruptMonitorCtx.createMediaStreamSource(interruptMonitorStream);
                interruptMonitorAnalyser = interruptMonitorCtx.createAnalyser();
                interruptMonitorAnalyser.fftSize = 2048; // Reliable RMS â€” 42ms window (512 was too noisy)
                source.connect(interruptMonitorAnalyser);
                interruptSpeechStart = null;

                interruptMonitorInterval = setInterval(() => {
                    if (!botIsPlaying && !waitingForBotAudio) {
                        // Bot finished all audio â€” hand off mic to real recording
                        const stream = interruptMonitorStream;
                        // Clear references but DON'T stop the stream â€” hand it off
                        clearInterval(interruptMonitorInterval);
                        interruptMonitorInterval = null;
                        if (interruptMonitorCtx) {
                            interruptMonitorCtx.close().catch(() => {});
                            interruptMonitorCtx = null;
                            interruptMonitorAnalyser = null;
                        }
                        interruptMonitorStream = null;
                        interruptSpeechStart = null;
                        if (autoListenEnabled && !isRecording) {
                            // Hand off the pre-warmed stream (zero mic startup delay)
                            startListening(stream);
                        } else if (stream) {
                            stream.getTracks().forEach(t => t.stop());
                        }
                        prewarmedMicStream = null;
                        return;
                    }
                    const rms = getRMS(interruptMonitorAnalyser);
                    if (rms > INTERRUPT_THRESHOLD) {
                        if (!interruptSpeechStart) interruptSpeechStart = Date.now();
                        // User has been speaking long enough â€” real interruption
                        if (Date.now() - interruptSpeechStart > INTERRUPT_SPEECH_MIN_MS) {
                            console.log('[Interrupt] User interrupted bot, RMS:', rms.toFixed(1));
                            // Blueprint: Interruption flow â€” cancel active LLM + TTS + flush buffer
                            socket.emit('cancel_stream');  // Signal backend to cancel generation
                            stopCurrentAudio();

                            // Grab the pre-warmed stream before clearing monitor state
                            const stream = interruptMonitorStream;
                            clearInterval(interruptMonitorInterval);
                            interruptMonitorInterval = null;
                            if (interruptMonitorCtx) {
                                interruptMonitorCtx.close().catch(() => {});
                                interruptMonitorCtx = null;
                                interruptMonitorAnalyser = null;
                            }
                            interruptMonitorStream = null;
                            interruptSpeechStart = null;

                            wasInterrupted = true;
                            statusEl.textContent = 'ðŸŽ¤ Go ahead, I\'m listening...';
                            // Hand off pre-warmed stream â€” ZERO getUserMedia delay
                            startListening(stream);
                            prewarmedMicStream = null;
                        }
                    } else {
                        interruptSpeechStart = null;
                    }
                }, VAD_CHECK_INTERVAL);
            } catch (e) {
                console.log('Interrupt monitor mic error:', e);
            }
        }

        function stopInterruptionMonitor() {
            if (interruptMonitorInterval) {
                clearInterval(interruptMonitorInterval);
                interruptMonitorInterval = null;
            }
            if (interruptMonitorStream) {
                interruptMonitorStream.getTracks().forEach(t => t.stop());
                interruptMonitorStream = null;
            }
            prewarmedMicStream = null;
            if (interruptMonitorCtx) {
                interruptMonitorCtx.close().catch(() => {});
                interruptMonitorCtx = null;
                interruptMonitorAnalyser = null;
            }
            interruptSpeechStart = null;
        }

        function getRMS(analyserNode) {
            if (!analyserNode) return 0;
            const data = new Uint8Array(analyserNode.fftSize);
            analyserNode.getByteTimeDomainData(data);
            let sum = 0;
            for (let i = 0; i < data.length; i++) {
                const val = (data[i] - 128);
                sum += val * val;
            }
            return Math.sqrt(sum / data.length);
        }

        function startVAD() {
            if (!analyser) return;
            silenceStart = null;
            speechDetected = false;
            speechStartTime = null;
            interruptionDetectedAt = null;
            vadInterval = setInterval(() => {
                // NEVER do VAD speech detection while bot is playing
                // (interruption is handled by the separate interruptionMonitor)
                if (botIsPlaying) return;

                const rms = getRMS(analyser);
                if (rms > SILENCE_THRESHOLD) {
                    // Voice detected
                    if (!speechDetected) {
                        speechStartTime = Date.now();
                    }
                    speechDetected = true;
                    silenceStart = null;
                    // Don't overwrite live transcription status if browser STT is showing it
                    if (!useBrowserSTT || !browserSTTResult) {
                        statusEl.textContent = 'ðŸŽ¤ Listening...';
                    }
                } else if (speechDetected) {
                    // Was speaking, now silent
                    if (!silenceStart) {
                        silenceStart = Date.now();
                    } else if (Date.now() - silenceStart > SILENCE_DURATION) {
                        stopRecordingAndSend();
                    }
                }
            }, VAD_CHECK_INTERVAL);
        }

        function stopVAD() {
            if (vadInterval) {
                clearInterval(vadInterval);
                vadInterval = null;
            }
        }

        // Stop any currently playing bot audio + flush streaming queue
        function stopCurrentAudio() {
            if (currentAudio) {
                try {
                    currentAudio.onended = null;
                    currentAudio.onerror = null;
                    currentAudio.pause();
                    currentAudio.removeAttribute('src');
                    currentAudio.load();  // Force release audio resource
                } catch(e) {}
                currentAudio = null;
            }
            // Flush streaming audio queue (blueprint: flush playback buffer)
            flushAudioQueue();
            botIsPlaying = false;
            waitingForBotAudio = false;
            // Reset streaming display state
            streamingTextBuffer = '';
            streamingBubbleEl = null;
            streamStartTime = null;
        }

        // Release microphone stream fully
        function releaseMicStream() {
            if (micStream) {
                micStream.getTracks().forEach(t => t.stop());
                micStream = null;
            }
            if (audioContext) {
                audioContext.close().catch(() => {});
                audioContext = null;
                analyser = null;
            }
        }

        // --- Socket events ---

        // ============================================================
        // STREAMING: Handle progressive text + audio chunks
        // Blueprint: Streaming TTS Layer + Progressive Playback Buffer
        // ============================================================
        socket.on('text_chunk', (data) => {
            // Don't process legacy streaming when Realtime API is active
            if (realtimeConnected) return;
            typingEl.style.display = 'none';

            if (!streamStartTime) streamStartTime = Date.now();

            // Pre-warm interrupt monitor on first text chunk (before audio arrives)
            // This gives ~500-1000ms head start for mic initialization
            if (data.chunk_index === 0 && autoListenEnabled && !interruptMonitorInterval) {
                waitingForBotAudio = true;
                startInterruptionMonitor();
            }

            // Progressive text display: append to existing bot bubble or create new one
            const chunkText = data.text;
            streamingTextBuffer += (streamingTextBuffer ? ' ' : '') + chunkText;

            if (!streamingBubbleEl) {
                // Create the bot message container on first chunk
                const div = document.createElement('div');
                div.className = 'message bot';
                div.id = 'streaming-msg';
                const safeText = escapeHTML(streamingTextBuffer);
                const icon = getBotIcon();
                div.innerHTML = `
                    <div class="avatar"><i class="fas ${icon}"></i></div>
                    <div>
                        <div class="bubble streaming-bubble">${safeText}</div>
                    </div>`;
                chatArea.appendChild(div);
                streamingBubbleEl = div.querySelector('.streaming-bubble');
            } else {
                // Update existing bubble with accumulated text
                streamingBubbleEl.textContent = streamingTextBuffer;
            }
            chatArea.scrollTop = chatArea.scrollHeight;
        });

        socket.on('audio_chunk', (data) => {
            // Don't process legacy audio when Realtime API handles audio via WebRTC
            if (realtimeConnected) return;
            typingEl.style.display = 'none';

            if (!streamStartTime) streamStartTime = Date.now();

            // Track time-to-first-audio
            if (data.chunk_index === 0) {
                const latency = Date.now() - (streamStartTime || Date.now());
                console.log(`[Streaming] First audio chunk latency: ${latency}ms`);
            }

            // Queue audio chunk for sequential playback
            enqueueAudioChunk(data.audio, data.chunk_index);

            // Clear pre-warm flag â€” audio is now being played
            waitingForBotAudio = false;

            // Start interruption monitor if not already running (fallback)
            if (data.chunk_index === 0 && autoListenEnabled && !interruptMonitorInterval) {
                startInterruptionMonitor();
            }
        });

        socket.on('stream_complete', (data) => {
            // Don't process legacy streaming when Realtime API is active
            if (realtimeConnected) return;
            // Streaming is done â€” finalize the message
            isProcessing = false;
            const totalTime = Date.now() - (streamStartTime || Date.now());
            console.log(`[Streaming] Complete: ${data.total_chunks} chunks, ${totalTime}ms`);

            // Add audio player to the completed message if we have audio
            if (streamingBubbleEl && streamingBubbleEl.parentElement) {
                // Message is already displayed progressively â€” nothing more needed
            }

            // Reset streaming state
            streamingTextBuffer = '';
            streamingBubbleEl = null;
            streamStartTime = null;
        });

        // Legacy: Full audio response (used for non-streaming fallback)
        socket.on('audio_response', (data) => {
            if (realtimeConnected) return;
            typingEl.style.display = 'none';
            // Stop any previous audio first
            stopCurrentAudio();
            flushAudioQueue();

            // Play audio
            const audioBytes = Uint8Array.from(atob(data.audio), c => c.charCodeAt(0));
            const blob = new Blob([audioBytes], { type: 'audio/ogg; codecs=opus' });
            const url = URL.createObjectURL(blob);

            // Add bot message with audio player
            addBotMessage(data.text, url);

            // Auto-play and track
            currentAudio = new Audio(url);
            botIsPlaying = true;
            currentAudio.onended = () => {
                currentAudio = null;
                botIsPlaying = false;
                // Auto-listen after bot finishes speaking (zero delay)
                if (autoListenEnabled && !isRecording) {
                    startListening();
                }
            };
            currentAudio.play().catch(e => {
                console.log('Autoplay blocked:', e);
                botIsPlaying = false;
                // Auto-listen since playback failed
                if (autoListenEnabled && !isRecording) {
                    startListening();
                }
            });

            // Start lightweight interruption monitor (NOT recording â€” just volume detection)
            if (autoListenEnabled) {
                startInterruptionMonitor();
            }
        });

        // COST OPTIMIZATION: Text-only response with on-demand play button
        socket.on('text_response', (data) => {
            typingEl.style.display = 'none';
            isProcessing = false;
            addBotMessageWithPlayBtn(data.text, data.msg_id);
        });

        // COST OPTIMIZATION: On-demand TTS audio received
        socket.on('tts_audio', (data) => {
            const msgId = data.msg_id;
            const audioBytes = Uint8Array.from(atob(data.audio), c => c.charCodeAt(0));
            const blob = new Blob([audioBytes], { type: 'audio/ogg; codecs=opus' });
            const url = URL.createObjectURL(blob);

            // Update the play button to show audio player
            const container = pendingTtsMessages[msgId];
            if (container) {
                const audioDiv = container.querySelector('.tts-play-area');
                if (audioDiv) {
                    audioDiv.innerHTML = `<audio controls autoplay src="${url}" style="height:32px;width:100%;margin-top:5px;"></audio>`;
                }
                delete pendingTtsMessages[msgId];
            }

            // Also play it
            stopCurrentAudio();
            currentAudio = new Audio(url);
            botIsPlaying = true;
            currentAudio.onended = () => {
                currentAudio = null;
                botIsPlaying = false;
            };
            currentAudio.play().catch(() => { botIsPlaying = false; });
        });

        socket.on('voice_mode_changed', (data) => {
            statusEl.textContent = data.message;
        });

        socket.on('status', (data) => {
            if (realtimeConnected) return;
            typingEl.style.display = 'none';
            isProcessing = false;
            statusEl.textContent = data.message;
            // Auto-restart listening after error/status if auto-listen is active
            if (autoListenEnabled && !isRecording && !botIsPlaying) {
                setTimeout(() => startListening(), 500);
            }
        });

        // Show user's transcribed text in chat (replaces voice message indicator)
        socket.on('user_transcription', (data) => {
            // Don't show legacy transcription when Realtime API handles it
            if (realtimeConnected) return;
            if (data.text) {
                addUserMessage(data.text);
            }
        });

        // --- UI functions ---
        function getBotIcon() {
            return (ROLE_LABELS[currentMode] || ROLE_LABELS.interview).icon;
        }

        function addBotMessage(text, audioUrl) {
            const div = document.createElement('div');
            div.className = 'message bot';
            // SECURITY: Escape text to prevent XSS
            const safeText = escapeHTML(text);
            const icon = getBotIcon();
            div.innerHTML = `
                <div class="avatar"><i class="fas ${icon}"></i></div>
                <div>
                    <div class="bubble">${safeText}</div>
                    ${audioUrl ? `<div class="audio-player"><audio controls src="${audioUrl}"></audio></div>` : ''}
                </div>`;
            chatArea.appendChild(div);
            chatArea.scrollTop = chatArea.scrollHeight;
            addTranscriptLine('bot', text);
        }

        function addUserMessage(text) {
            const div = document.createElement('div');
            div.className = 'message user';
            // SECURITY: Escape text to prevent XSS
            const safeText = escapeHTML(text);
            div.innerHTML = `
                <div class="bubble">${safeText}</div>
                <div class="avatar"><i class="fas fa-user"></i></div>`;
            chatArea.appendChild(div);
            chatArea.scrollTop = chatArea.scrollHeight;
            addTranscriptLine('user', text);
        }

        // COST OPTIMIZATION: Bot message with on-demand play button (no auto TTS)
        function addBotMessageWithPlayBtn(text, msgId) {
            const div = document.createElement('div');
            div.className = 'message bot';
            const safeText = escapeHTML(text);
            const encodedText = btoa(encodeURIComponent(text));
            const icon = getBotIcon();
            div.innerHTML = `
                <div class="avatar"><i class="fas ${icon}"></i></div>
                <div>
                    <div class="bubble">${safeText}</div>
                    <div class="tts-play-area">
                        <button class="play-tts-btn" data-msgid="${msgId}" data-encoded="${encodedText}">
                            <i class="fas fa-volume-up"></i> Play audio
                        </button>
                    </div>
                </div>`;
            // Attach click handler safely (avoids inline JS injection)
            const playBtn = div.querySelector('.play-tts-btn');
            playBtn.addEventListener('click', function() {
                requestTTS(this, msgId, encodedText);
            });
            chatArea.appendChild(div);
            chatArea.scrollTop = chatArea.scrollHeight;
            // Store reference for when TTS comes back
            pendingTtsMessages[msgId] = div;
        }

        // Request on-demand TTS for a specific message
        function requestTTS(btn, msgId, encodedText) {
            const text = decodeURIComponent(atob(encodedText));
            btn.disabled = true;
            btn.innerHTML = '<i class="fas fa-spinner fa-spin"></i> Generating...';
            socket.emit('request_tts', { text: text, msg_id: msgId });
        }

        function showThinking() {
            // Set role-specific thinking label
            const label = document.getElementById('thinking-label');
            const role = ROLE_LABELS[currentMode] || ROLE_LABELS.interview;
            label.textContent = role.thinking;
            typingEl.style.display = 'block';
            chatArea.scrollTop = chatArea.scrollHeight;
            isProcessing = true;
        }

        // ============================================================
        // FILE UPLOAD: CV & Job Profile for interview context
        // ============================================================
        async function handleFileUpload(type) {
            const fileInput = document.getElementById(type === 'cv' ? 'cv-upload' : 'job-upload');
            const statusEl = document.getElementById(type === 'cv' ? 'cv-status' : 'job-status');
            const file = fileInput.files[0];
            if (!file) return;

            // Validate file size (max 5MB)
            if (file.size > 5 * 1024 * 1024) {
                statusEl.textContent = 'âŒ File too large (max 5MB)';
                statusEl.style.color = '#dc3545';
                fileInput.value = '';
                return;
            }

            statusEl.textContent = 'â³ Uploading...';
            statusEl.style.color = '#856404';

            const formData = new FormData();
            formData.append('file', file);
            formData.append('type', type);

            try {
                const res = await fetch('/api/upload-document', {
                    method: 'POST',
                    body: formData,
                });
                const data = await res.json();
                if (res.ok && data.text) {
                    if (type === 'cv') uploadedCV = data.text;
                    else uploadedJobProfile = data.text;
                    statusEl.innerHTML = 'âœ… ' + file.name + ' <span class="file-remove" onclick="removeUpload(\'' + type + '\')">âœ• remove</span>';
                    statusEl.style.color = '#28a745';
                } else {
                    statusEl.textContent = 'âŒ ' + (data.error || 'Upload failed');
                    statusEl.style.color = '#dc3545';
                    fileInput.value = '';
                }
            } catch(e) {
                statusEl.textContent = 'âŒ Upload error';
                statusEl.style.color = '#dc3545';
                fileInput.value = '';
            }
        }

        function removeUpload(type) {
            if (type === 'cv') {
                uploadedCV = '';
                document.getElementById('cv-upload').value = '';
                document.getElementById('cv-status').textContent = '';
            } else {
                uploadedJobProfile = '';
                document.getElementById('job-upload').value = '';
                document.getElementById('job-status').textContent = '';
            }
        }

        // ============================================================
        // VIDEO AVATAR: Show animated interviewer/agent
        // ============================================================
        function showVideoAvatar(mode) {
            const container = document.getElementById('video-avatar');
            const svgImg = document.getElementById('avatar-svg');
            const fallback = document.getElementById('avatar-fallback');
            const nameEl = document.getElementById('avatar-name');
            const role = ROLE_LABELS[mode] || ROLE_LABELS.interview;

            // Pick a random persona if not already set for this session
            if (!currentPersona) pickRandomPersona(mode);
            const persona = currentPersona;

            nameEl.textContent = persona.name + ' â€” ' + persona.title;
            fallback.className = 'fas ' + persona.icon + ' avatar-fallback';

            // Always show the avatar container
            container.style.display = 'block';

            // Load SVG avatar image
            if (persona.avatar) {
                svgImg.src = persona.avatar;
                svgImg.onload = () => {
                    svgImg.style.display = 'block';
                    fallback.style.display = 'none';
                };
                svgImg.onerror = () => {
                    svgImg.style.display = 'none';
                    fallback.style.display = 'block';
                };
            } else {
                svgImg.style.display = 'none';
                fallback.style.display = 'block';
            }
            console.log('[Avatar] Showing persona:', persona.name, 'mode:', mode, 'avatar:', persona.avatar);
        }

        function hideVideoAvatar() {
            document.getElementById('video-avatar').style.display = 'none';
            const svgImg = document.getElementById('avatar-svg');
            svgImg.style.display = 'none';
            document.getElementById('avatar-fallback').style.display = 'block';
            setAvatarSpeaking(false);
        }

        function setAvatarSpeaking(isSpeaking) {
            const ring = document.getElementById('speaking-ring');
            if (isSpeaking) ring.classList.add('active');
            else ring.classList.remove('active');
        }

        // ============================================================
        // LIVE TRANSCRIPT: Show text for both candidate and bot
        // ============================================================
        function showTranscriptPanel() {
            document.getElementById('transcript-panel').style.display = 'block';
        }

        function hideTranscriptPanel() {
            document.getElementById('transcript-panel').style.display = 'none';
            document.getElementById('transcript-content').innerHTML = '';
        }

        function addTranscriptLine(role, text) {
            const panel = document.getElementById('transcript-panel');
            const content = document.getElementById('transcript-content');
            panel.style.display = 'block';

            const roleName = role === 'user' ? 'You' : (ROLE_LABELS[currentMode] || ROLE_LABELS.interview).name.split(' â€” ')[0];
            const cssClass = role === 'user' ? 'user-line' : 'bot-line';
            const line = document.createElement('div');
            line.className = 'transcript-line ' + cssClass;
            line.innerHTML = '<span class=\"transcript-label\">' + escapeHTML(roleName) + ':</span> ' + escapeHTML(text);
            content.appendChild(line);

            // Keep only last 20 lines
            while (content.children.length > 20) content.removeChild(content.firstChild);
            panel.scrollTop = panel.scrollHeight;
        }

        function updateTranscriptLast(text) {
            const content = document.getElementById('transcript-content');
            if (content.lastChild) {
                const label = content.lastChild.querySelector('.transcript-label');
                const labelText = label ? label.outerHTML : '';
                content.lastChild.innerHTML = labelText + ' ' + escapeHTML(text);
            }
            const panel = document.getElementById('transcript-panel');
            panel.scrollTop = panel.scrollHeight;
        }

        // --- Active mode highlight ---
        function setActiveMode(modeId) {
            // Remove active from all mode buttons
            document.querySelectorAll('.mode-btn').forEach(btn => btn.classList.remove('active'));
            // Add active to selected
            if (modeId) {
                const btn = document.getElementById(modeId);
                if (btn) btn.classList.add('active');
            }
        }

        // --- End button: green when call active, red when ended ---
        function setCallActive(isActive) {
            const btn = document.getElementById('btn-reset');
            const icon = document.getElementById('end-icon');
            const label = document.getElementById('end-label');
            if (isActive) {
                btn.classList.add('call-active');
                icon.className = 'fas fa-phone';
                label.textContent = 'End Call';
            } else {
                btn.classList.remove('call-active');
                icon.className = 'fas fa-phone-slash';
                label.textContent = 'End';
            }
        }

        // --- Mode selection ---
        // Cleanup before switching modes â€” cancel streams, stop audio, stop recording
        function cleanupBeforeModeSwitch() {
            // Disconnect Realtime API if active
            disconnectRealtime();
            // Legacy cleanup
            socket.emit('cancel_stream');
            stopCurrentAudio();
            flushAudioQueue();
            stopVAD();
            stopBrowserSTT();
            stopInterruptionMonitor();
            if (isRecording && mediaRecorder) {
                try { mediaRecorder.stop(); } catch(e) {}
                isRecording = false;
                micBtn.classList.remove('recording');
            }
            releaseMicStream();
            wasInterrupted = false;
            waitingForBotAudio = false;
            isProcessing = false;
        }

        function startInterview() {
            // End any active call before showing setup
            cleanupBeforeModeSwitch();
            hideVideoAvatar();
            hideTranscriptPanel();
            chatArea.innerHTML = '';
            currentPersona = null;
            // Show the interview setup panel (CV/Job upload)
            currentMode = 'interview';
            setActiveMode('btn-interview');
            setCallActive(false);
            document.getElementById('interview-setup').style.display = 'block';
            document.getElementById('language-picker').style.display = 'none';
            statusEl.textContent = 'Upload your CV/Job Profile (optional) or start directly';
        }

        function startInterviewWithContext() {
            document.getElementById('interview-setup').style.display = 'none';
            cleanupBeforeModeSwitch();
            chatArea.innerHTML = '';
            statusEl.textContent = 'Interview Assistant';
            currentMode = 'interview';
            currentPersona = null; // Reset so new persona is picked
            pickRandomPersona('interview');
            setActiveMode('btn-interview');
            setCallActive(true);
            startSessionTimer();
            showVideoAvatar('interview');
            showTranscriptPanel();

            if (voiceModeOn) {
                connectRealtime('interview', 'en', uploadedCV, uploadedJobProfile);
            } else {
                socket.emit('toggle_voice_mode', { voice_mode: false });
                showThinking();
                socket.emit('start_interview', { cv_text: uploadedCV, job_profile_text: uploadedJobProfile });
            }
        }

        function startHelpdesk() {
            cleanupBeforeModeSwitch();
            document.getElementById('interview-setup').style.display = 'none';
            chatArea.innerHTML = '';
            statusEl.textContent = 'IT Helpdesk';
            currentMode = 'helpdesk';
            currentPersona = null;
            pickRandomPersona('helpdesk');
            setActiveMode('btn-helpdesk');
            setCallActive(true);
            startSessionTimer();
            showVideoAvatar('helpdesk');
            showTranscriptPanel();

            if (voiceModeOn) {
                connectRealtime('helpdesk');
            } else {
                socket.emit('toggle_voice_mode', { voice_mode: false });
                showThinking();
                socket.emit('start_helpdesk');
            }
        }

        function showLanguagePicker() {
            // End any active call before showing language picker
            cleanupBeforeModeSwitch();
            hideVideoAvatar();
            hideTranscriptPanel();
            chatArea.innerHTML = '';
            currentPersona = null;
            currentMode = 'language';
            setActiveMode('btn-language');
            setCallActive(false);
            document.getElementById('language-picker').style.display = 'block';
            document.getElementById('interview-setup').style.display = 'none';
            statusEl.textContent = 'Select a language to practice';
        }

        function startLanguageTest() {
            cleanupBeforeModeSwitch();
            const lang = document.getElementById('lang-select').value;
            currentLanguage = lang;
            document.getElementById('language-picker').style.display = 'none';
            chatArea.innerHTML = '';
            statusEl.textContent = 'Language Practice Companion';
            currentPersona = null;
            pickRandomPersona('language');
            setCallActive(true);
            startSessionTimer();
            showVideoAvatar('language');
            showTranscriptPanel();

            if (voiceModeOn) {
                connectRealtime('language', lang);
            } else {
                socket.emit('toggle_voice_mode', { voice_mode: false });
                showThinking();
                socket.emit('start_language_test', { language: lang });
            }
        }

        function resetSession() {
            // Disconnect Realtime API
            disconnectRealtime();
            setCallActive(false);
            currentPersona = null; // Reset persona for next session
            // Stop all audio, streaming, recording, and browser STT
            socket.emit('cancel_stream');
            stopCurrentAudio();
            flushAudioQueue();
            stopVAD();
            stopBrowserSTT();
            stopInterruptionMonitor();
            if (typeof stopTTSPlayback === 'function') stopTTSPlayback();
            if (isRecording && mediaRecorder) {
                try { mediaRecorder.stop(); } catch(e) {}
                isRecording = false;
                micBtn.classList.remove('recording');
            }
            releaseMicStream();
            // Reset audio/recording state
            autoListenEnabled = true;
            wasInterrupted = false;
            speechDetected = false;
            silenceStart = null;
            voiceModeOn = true;
            document.getElementById('voice-toggle').checked = true;
            document.getElementById('voice-mode-label').textContent = 'Voice mode';
            document.getElementById('cost-badge').textContent = 'realtime AI';
            document.getElementById('cost-badge').className = 'cost-badge active';
            if (sessionTimerInterval) clearInterval(sessionTimerInterval);
            document.getElementById('session-timer').textContent = '00:00';
            pendingTtsMessages = {};
            chatArea.innerHTML = '';
            document.getElementById('language-picker').style.display = 'none';
            document.getElementById('interview-setup').style.display = 'none';
            hideVideoAvatar();
            hideTranscriptPanel();
            // Send reset with current mode so backend re-initializes correctly
            socket.emit('reset', { mode: currentMode, language: currentLanguage });

            // Stay in the same module â€” keep button highlighted, show ready status
            if (currentMode === 'interview') {
                setActiveMode('btn-interview');
                statusEl.textContent = 'Interview Assistant â€” tap mic or type to start';
            } else if (currentMode === 'helpdesk') {
                setActiveMode('btn-helpdesk');
                statusEl.textContent = 'IT Helpdesk â€” tap mic or type to start';
            } else if (currentMode === 'language') {
                setActiveMode('btn-language');
                statusEl.textContent = 'Language Practice â€” tap mic or type to start';
            } else {
                setActiveMode(null);
                statusEl.textContent = 'Choose a mode to start';
            }
        }

        // --- Text input ---
        function sendText() {
            if (!isConnected && !realtimeConnected) {
                statusEl.textContent = 'Not connected â€” please wait...';
                return;
            }
            let text = textInput.value.trim();
            if (!text) return;
            // SECURITY: Client-side length limit
            if (text.length > 2000) {
                text = text.substring(0, 2000);
                statusEl.textContent = 'Message trimmed to 2000 characters.';
            }

            if (realtimeConnected && realtimeDc) {
                // REALTIME MODE: Send text through WebRTC data channel
                addUserMessage(text);
                textInput.value = '';
                // Create a conversation item with the text
                realtimeDc.send(JSON.stringify({
                    type: 'conversation.item.create',
                    item: {
                        type: 'message',
                        role: 'user',
                        content: [{ type: 'input_text', text: text }],
                    },
                }));
                // Trigger response generation
                realtimeDc.send(JSON.stringify({
                    type: 'response.create',
                    response: { modalities: ['audio', 'text'] },
                }));
                // Log to backend
                socket.emit('realtime_log', { role: 'user', text: text, mode: currentMode });
            } else {
                // LEGACY MODE: Socket.IO text
                socket.emit('cancel_stream');
                stopCurrentAudio();
                addUserMessage(text);
                textInput.value = '';
                showThinking();
                socket.emit('text_message', { text: text, mode: currentMode });
            }
        }

        // --- Audio recording with Voice Activity Detection ---
        async function startListening(existingStream) {
            // Don't run legacy recording when Realtime API handles everything
            if (realtimeConnected) return;
            if (isRecording) return;
            // If bot is still playing, don't start recording (use interrupt monitor instead)
            if (botIsPlaying) return;
            // Stop any lingering interrupt monitor
            stopInterruptionMonitor();

            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                statusEl.textContent = 'Browser does not support audio recording';
                return;
            }
            try {
                // Reuse pre-warmed or existing mic stream (avoids 100-300ms getUserMedia delay)
                if (existingStream && existingStream.active) {
                    micStream = existingStream;
                    console.log('[Mic] Reusing mic stream (zero acquisition delay)');
                } else {
                    // Release dead stream if any before acquiring new one
                    if (micStream) { micStream.getTracks().forEach(t => t.stop()); micStream = null; }
                    micStream = await navigator.mediaDevices.getUserMedia({
                        audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: true }
                    });
                }

                // Close old AudioContext to prevent resource leaks
                if (audioContext) {
                    audioContext.close().catch(() => {});
                    audioContext = null;
                    analyser = null;
                }

                // Set up Web Audio API for VAD
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(micStream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 2048; // Reliable RMS â€” 42ms window (512 was only 10ms, too noisy)
                source.connect(analyser);

                // Set up MediaRecorder
                const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
                    ? 'audio/webm;codecs=opus' : 'audio/webm';
                mediaRecorder = new MediaRecorder(micStream, { mimeType });
                audioChunks = [];

                mediaRecorder.ondataavailable = e => {
                    if (e.data.size > 0) audioChunks.push(e.data);
                };

                mediaRecorder.onstop = () => {
                    stopVAD();

                    // ============================================================
                    // FAST PATH: Use browser SpeechRecognition text (instant)
                    // SLOW PATH: Use Whisper via audio upload (fallback)
                    // ============================================================
                    const browserText = getBrowserSTTText();
                    stopBrowserSTT();

                    // If still processing a previous response, cancel it first
                    if (isProcessing) {
                        socket.emit('cancel_stream');
                        stopCurrentAudio();
                        isProcessing = false;
                        wasInterrupted = true;
                    }

                    if (browserText && browserText.length > 1) {
                        // FAST PATH: Browser already transcribed â€” send as text instantly
                        console.log('[STT] Using browser transcription:', browserText.substring(0, 50));
                        addUserMessage(browserText);
                        showThinking();
                        socket.emit('text_message', { text: browserText, interrupted: wasInterrupted, mode: currentMode });
                        wasInterrupted = false;
                        // Release mic â€” bot will respond with interrupt monitor
                        releaseMicStream();
                    } else {
                        // SLOW PATH: No browser STT text â€” use Whisper (upload audio)
                        const blob = new Blob(audioChunks, { type: mimeType });
                        if (speechDetected && blob.size > 2000) {
                            const reader = new FileReader();
                            reader.onload = () => {
                                const b64 = btoa(
                                    new Uint8Array(reader.result)
                                        .reduce((data, byte) => data + String.fromCharCode(byte), '')
                                );
                                // Don't show user message here â€” backend will emit transcription
                                showThinking();
                                socket.emit('audio_message', { audio: b64, mimeType: mimeType, interrupted: wasInterrupted, mode: currentMode });
                                wasInterrupted = false;
                            };
                            reader.readAsArrayBuffer(blob);
                            // Release mic â€” bot will respond with interrupt monitor
                            releaseMicStream();
                        } else {
                            // NO SPEECH â€” reuse mic stream for instant restart (no getUserMedia gap)
                            if (autoListenEnabled && !botIsPlaying) {
                                statusEl.textContent = 'ðŸŽ¤ Waiting for you to speak...';
                                // Pass existing mic â€” avoids 100-300ms getUserMedia delay
                                startListening(micStream);
                                return; // Don't release mic â€” startListening reuses it
                            }
                            if (!autoListenEnabled) {
                                statusEl.textContent = 'No speech detected. Tap mic to try again.';
                            }
                            releaseMicStream();
                        }
                    }
                };

                mediaRecorder.start(250);
                isRecording = true;
                autoListenEnabled = true;
                micBtn.classList.add('recording');
                statusEl.textContent = 'ðŸŽ¤ Listening... (speak, I\'ll detect when you stop)';

                // Start browser SpeechRecognition for instant transcription (English only)
                if (useBrowserSTT && currentMode !== 'language') {
                    startBrowserSTT();
                }

                // Start voice activity detection
                startVAD();

            } catch (err) {
                if (err.name === 'NotFoundError') {
                    statusEl.textContent = 'No microphone found. Please connect one.';
                } else if (err.name === 'NotAllowedError') {
                    statusEl.textContent = 'Microphone permission denied. Allow it in browser settings.';
                } else {
                    statusEl.textContent = 'Mic error: ' + err.message;
                }
            }
        }

        function stopRecordingAndSend() {
            if (!isRecording || !mediaRecorder) return;
            mediaRecorder.stop();
            isRecording = false;
            micBtn.classList.remove('recording');
            // If browser STT is active, show instant status (no "processing" delay)
            if (useBrowserSTT && browserSTTResult) {
                statusEl.textContent = 'âš¡ Sending...';
            } else {
                statusEl.textContent = 'Processing audio...';
            }
        }

        // Toggle: click mic to start/stop voice, or mute/unmute in Realtime mode
        async function toggleRecording() {
            if (realtimeConnected) {
                // REALTIME MODE: Mute/unmute mic
                toggleRealtimeMute();
                return;
            }
            // LEGACY MODE: Start/stop recording with VAD
            if (!isRecording) {
                await startListening();
            } else {
                // Manual stop (force-stop override)
                stopRecordingAndSend();
            }
        }

        // --- PWA: Register Service Worker ---
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', () => {
                navigator.serviceWorker.register('/sw.js', { scope: '/' })
                    .then((reg) => console.log('[PWA] Service Worker registered, scope:', reg.scope))
                    .catch((err) => console.log('[PWA] SW registration failed:', err));
            });
        }

        // --- Auto-start mode from dashboard selection ---
        (function autoStartMode() {
            const urlMode = '{{ mode }}';
            if (urlMode === 'interview') {
                setTimeout(() => startInterview(), 300);
            } else if (urlMode === 'language') {
                setTimeout(() => showLanguagePicker(), 300);
            } else if (urlMode === 'helpdesk') {
                setTimeout(() => startHelpdesk(), 300);
            }
        })();
    </script>
    <script src="/static/voice.js"></script>
</body>
</html>