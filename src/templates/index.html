<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta name="description" content="AI-powered interview preparation and language practice assistant">
    <meta name="theme-color" content="#667eea">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="apple-mobile-web-app-title" content="Interview Prep">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <title>Interview Preparation Assistant</title>
    <!-- PWA -->
    <link rel="manifest" href="/static/manifest.json">
    <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
    <link rel="apple-touch-icon" sizes="192x192" href="/static/icons/icon-192x192.png">
    <link rel="apple-touch-icon" sizes="512x512" href="/static/icons/icon-512x512.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
    <style>
        * { box-sizing: border-box; margin: 0; padding: 0; }
        html, body {
            height: 100%;
            overflow: hidden;
        }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            display: flex;
            justify-content: center;
            align-items: center;
        }
        .container {
            width: 100%;
            max-width: 600px;
            height: 100vh;
            max-height: 100vh;
            background: #fff;
            overflow: hidden;
            display: flex;
            flex-direction: column;
        }
        @media (min-height: 700px) and (min-width: 640px) {
            .container {
                height: 95vh;
                max-height: 95vh;
                border-radius: 20px;
                box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            }
        }
        @media (max-width: 639px) {
            .container {
                max-width: 100%;
                border-radius: 0;
            }
        }
        .header {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 15px 20px;
            text-align: center;
            flex-shrink: 0;
        }
        .header h1 { font-size: 20px; margin-bottom: 3px; }
        .header p { font-size: 12px; opacity: 0.85; }
        .mode-buttons {
            display: flex;
            gap: 8px;
            padding: 12px 15px;
            justify-content: center;
            flex-wrap: wrap;
            flex-shrink: 0;
        }
        .mode-btn {
            padding: 12px 20px;
            border: none;
            border-radius: 25px;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s;
            color: white;
        }
        .mode-btn.interview { background: #007bff; }
        .mode-btn.interview:hover { background: #0056b3; }
        .mode-btn.language { background: #28a745; }
        .mode-btn.language:hover { background: #1e7e34; }
        .mode-btn.helpdesk { background: #17a2b8; }
        .mode-btn.helpdesk:hover { background: #117a8b; }
        .mode-btn.reset { background: #dc3545; font-size: 12px; padding: 8px 15px; }
        .mode-btn.reset:hover { background: #c82333; }
        /* Active mode highlight */
        .mode-btn.active {
            outline: 3px solid #FFD700;
            box-shadow: 0 0 0 4px rgba(255,215,0,0.5), 0 4px 15px rgba(0,0,0,0.3);
            transform: scale(1.1);
            filter: brightness(1.2);
            position: relative;
        }
        .mode-btn.active::after {
            content: '\f00c';
            font-family: 'Font Awesome 5 Free';
            font-weight: 900;
            position: absolute;
            top: -6px;
            right: -6px;
            background: #FFD700;
            color: #333;
            border-radius: 50%;
            width: 18px;
            height: 18px;
            font-size: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        #language-picker {
            display: none;
            padding: 0 20px 15px;
            text-align: center;
        }
        #language-picker select {
            padding: 10px 15px;
            border-radius: 10px;
            border: 2px solid #28a745;
            font-size: 14px;
            margin-right: 10px;
        }
        #language-picker button {
            padding: 10px 20px;
            background: #28a745;
            color: white;
            border: none;
            border-radius: 10px;
            font-size: 14px;
            cursor: pointer;
        }

        .chat-area {
            flex: 1;
            min-height: 0;
            overflow-y: auto;
            padding: 15px;
            background: #f8f9fa;
        }
        .message {
            margin-bottom: 15px;
            display: flex;
            align-items: flex-start;
            gap: 10px;
        }
        .message.bot { justify-content: flex-start; }
        .message.user { justify-content: flex-end; }
        .message .avatar {
            width: 36px; height: 36px;
            border-radius: 50%;
            display: flex; align-items: center; justify-content: center;
            font-size: 16px; color: white; flex-shrink: 0;
        }
        .message.bot .avatar { background: #667eea; }
        .message.user .avatar { background: #28a745; }
        .bubble {
            max-width: 75%;
            padding: 12px 16px;
            border-radius: 18px;
            font-size: 14px;
            line-height: 1.5;
        }
        .message.bot .bubble {
            background: white;
            color: #333;
            border-bottom-left-radius: 4px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }
        .message.user .bubble {
            background: #007bff;
            color: white;
            border-bottom-right-radius: 4px;
        }
        .message .audio-player {
            margin-top: 8px;
        }
        .message .audio-player audio {
            height: 32px;
            width: 100%;
        }

        .input-area {
            display: flex;
            padding: 10px 15px;
            gap: 8px;
            border-top: 1px solid #eee;
            background: white;
            align-items: center;
            flex-shrink: 0;
        }
        .input-area input {
            flex: 1;
            padding: 12px 16px;
            border: 2px solid #e0e0e0;
            border-radius: 25px;
            font-size: 14px;
            outline: none;
            transition: border-color 0.3s;
        }
        .input-area input:focus { border-color: #667eea; }
        .send-btn {
            width: 44px; height: 44px;
            border-radius: 50%;
            border: none;
            background: #007bff;
            color: white;
            font-size: 18px;
            cursor: pointer;
            transition: background 0.3s;
        }
        .send-btn:hover { background: #0056b3; }
        .mic-btn {
            width: 50px; height: 50px;
            border-radius: 50%;
            border: none;
            background: #28a745;
            color: white;
            font-size: 20px;
            cursor: pointer;
            transition: all 0.3s;
        }
        .mic-btn.recording {
            background: #28a745;
            animation: pulse 1s infinite;
            box-shadow: 0 0 20px rgba(40,167,69,0.5);
        }
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.15); }
            100% { transform: scale(1); }
        }
        .status-bar {
            text-align: center;
            padding: 5px;
            font-size: 11px;
            color: #666;
            background: #f0f0f0;
            flex-shrink: 0;
        }
        .typing-indicator {
            display: none;
            padding: 10px 20px;
            color: #999;
            font-size: 13px;
            font-style: italic;
        }
        /* Voice/Text toggle */
        .voice-toggle-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 6px 15px;
            flex-shrink: 0;
            background: #f0f0f0;
            border-top: 1px solid #e0e0e0;
        }
        .voice-toggle {
            display: flex;
            align-items: center;
            gap: 8px;
            font-size: 13px;
            color: #555;
        }
        .voice-toggle label {
            cursor: pointer;
            display: flex;
            align-items: center;
            gap: 6px;
        }
        .toggle-switch {
            position: relative;
            width: 42px;
            height: 22px;
            display: inline-block;
        }
        .toggle-switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }
        .toggle-slider {
            position: absolute;
            cursor: pointer;
            top: 0; left: 0; right: 0; bottom: 0;
            background-color: #ccc;
            transition: 0.3s;
            border-radius: 22px;
        }
        .toggle-slider:before {
            position: absolute;
            content: "";
            height: 16px;
            width: 16px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: 0.3s;
            border-radius: 50%;
        }
        .toggle-switch input:checked + .toggle-slider {
            background-color: #28a745;
        }
        .toggle-switch input:checked + .toggle-slider:before {
            transform: translateX(20px);
        }
        .session-timer {
            font-size: 12px;
            color: #888;
            font-family: monospace;
        }
        /* Play button for on-demand TTS */
        .play-tts-btn {
            background: none;
            border: 1px solid #667eea;
            color: #667eea;
            border-radius: 15px;
            padding: 4px 10px;
            font-size: 12px;
            cursor: pointer;
            margin-top: 5px;
            transition: all 0.2s;
        }
        .play-tts-btn:hover {
            background: #667eea;
            color: white;
        }
        .play-tts-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }
        .play-tts-btn i {
            margin-right: 4px;
        }
        .cost-badge {
            font-size: 10px;
            padding: 2px 6px;
            border-radius: 8px;
            margin-left: 6px;
        }
        .cost-badge.saving { background: #d4edda; color: #155724; }
        .cost-badge.active { background: #fff3cd; color: #856404; }

        /* Interview Setup Panel */
        .interview-setup {
            display: none;
            padding: 12px 15px;
            background: #f0f4ff;
            border-bottom: 1px solid #dde;
            flex-shrink: 0;
        }
        .interview-setup h3 {
            font-size: 14px;
            color: #333;
            margin-bottom: 8px;
        }
        .upload-row {
            display: flex;
            gap: 8px;
            margin-bottom: 8px;
            align-items: center;
        }
        .upload-row label {
            font-size: 12px;
            color: #555;
            min-width: 90px;
            font-weight: 600;
        }
        .upload-row .file-input-wrapper {
            flex: 1;
            position: relative;
        }
        .upload-row input[type="file"] {
            font-size: 12px;
            width: 100%;
        }
        .upload-row .file-status {
            font-size: 11px;
            color: #28a745;
            margin-left: 5px;
        }
        .upload-row .file-remove {
            font-size: 11px;
            color: #dc3545;
            cursor: pointer;
            margin-left: 5px;
            text-decoration: underline;
        }
        .setup-start-btn {
            width: 100%;
            padding: 10px;
            border: none;
            border-radius: 10px;
            background: #007bff;
            color: white;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: background 0.3s;
        }
        .setup-start-btn:hover { background: #0056b3; }
        .setup-start-btn:disabled {
            background: #ccc;
            cursor: not-allowed;
        }
        .optional-tag {
            font-size: 10px;
            color: #999;
            font-weight: normal;
        }

        /* Video Avatar */
        .video-avatar-container {
            display: none;
            text-align: center;
            padding: 8px 15px;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            flex-shrink: 0;
            position: relative;
        }
        .avatar-visual {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            margin: 0 auto;
            position: relative;
            overflow: hidden;
            border: 3px solid #667eea;
            background: linear-gradient(135deg, #667eea, #764ba2);
            display: flex;
            align-items: center;
            justify-content: center;
        }
        .avatar-visual video {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        .avatar-visual .avatar-fallback {
            font-size: 42px;
            color: white;
        }
        .video-avatar-container .avatar-name {
            color: #ccc;
            font-size: 12px;
            margin-top: 4px;
        }
        .video-avatar-container .speaking-ring {
            position: absolute;
            top: 8px;
            left: 50%;
            transform: translateX(-50%);
            width: 110px;
            height: 110px;
            border-radius: 50%;
            border: 3px solid transparent;
            transition: border-color 0.3s;
            pointer-events: none;
        }
        .video-avatar-container .speaking-ring.active {
            border-color: #28a745;
            animation: avatar-pulse 1.5s infinite;
        }
        @keyframes avatar-pulse {
            0% { box-shadow: 0 0 0 0 rgba(40,167,69,0.4); }
            70% { box-shadow: 0 0 0 12px rgba(40,167,69,0); }
            100% { box-shadow: 0 0 0 0 rgba(40,167,69,0); }
        }

        /* Transcript Panel */
        .transcript-panel {
            display: none;
            max-height: 60px;
            overflow-y: auto;
            padding: 6px 15px;
            background: rgba(0,0,0,0.03);
            border-top: 1px solid #eee;
            flex-shrink: 0;
            font-size: 12px;
            color: #555;
        }
        .transcript-panel .transcript-line {
            margin-bottom: 3px;
            line-height: 1.4;
        }
        .transcript-panel .transcript-line.user-line {
            color: #007bff;
        }
        .transcript-panel .transcript-line.bot-line {
            color: #333;
        }
        .transcript-panel .transcript-label {
            font-weight: 600;
            font-size: 11px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1><i class="fas fa-headset"></i> Interview Prep Assistant</h1>
            <p>Practice interviews, language speaking & IT support with AI</p>
        </div>

        <div class="mode-buttons" id="mode-buttons">
            <button class="mode-btn interview" id="btn-interview" onclick="startInterview()">
                <i class="fas fa-briefcase"></i> Interview
            </button>
            <button class="mode-btn language" id="btn-language" onclick="showLanguagePicker()">
                <i class="fas fa-language"></i> Language
            </button>
            <button class="mode-btn helpdesk" id="btn-helpdesk" onclick="startHelpdesk()">
                <i class="fas fa-headset"></i> Helpdesk
            </button>
            <button class="mode-btn reset" id="btn-reset" onclick="resetSession()">
                <i class="fas fa-phone-slash"></i> End
            </button>
        </div>

        <div id="language-picker">
            <select id="lang-select">
                <option value="en">English</option>
                <option value="es">Spanish</option>
                <option value="fr">French</option>
                <option value="de">German</option>
                <option value="zh">Chinese</option>
                <option value="hi">Hindi</option>
                <option value="ja">Japanese</option>
                <option value="ko">Korean</option>
                <option value="pt">Portuguese</option>
                <option value="ar">Arabic</option>
                <option value="ru">Russian</option>
                <option value="it">Italian</option>
                <option value="nl">Dutch</option>
            </select>
            <button onclick="startLanguageTest()">Start</button>
        </div>

        <!-- Interview Setup Panel (CV + Job Profile upload) -->
        <div class="interview-setup" id="interview-setup">
            <h3><i class="fas fa-file-upload"></i> Prepare for Interview</h3>
            <div class="upload-row">
                <label>CV / Resume <span class="optional-tag">(optional)</span></label>
                <div class="file-input-wrapper">
                    <input type="file" id="cv-upload" accept=".pdf,.doc,.docx,.txt" onchange="handleFileUpload('cv')">
                    <span class="file-status" id="cv-status"></span>
                </div>
            </div>
            <div class="upload-row">
                <label>Job Profile <span class="optional-tag">(optional)</span></label>
                <div class="file-input-wrapper">
                    <input type="file" id="job-upload" accept=".pdf,.doc,.docx,.txt" onchange="handleFileUpload('job')">
                    <span class="file-status" id="job-status"></span>
                </div>
            </div>
            <button class="setup-start-btn" id="setup-start-btn" onclick="startInterviewWithContext()">
                <i class="fas fa-play"></i> Start Interview
            </button>
        </div>

        <!-- Video Avatar for Interviewer -->
        <div class="video-avatar-container" id="video-avatar">
            <div class="speaking-ring" id="speaking-ring"></div>
            <div class="avatar-visual" id="avatar-visual">
                <video id="avatar-video" loop muted playsinline style="display:none;">
                    <source src="/static/interviewer.mp4" type="video/mp4">
                </video>
                <i class="fas fa-user-tie avatar-fallback" id="avatar-fallback"></i>
            </div>
            <div class="avatar-name" id="avatar-name">Charlotte ‚Äî Interviewer</div>
        </div>

        <div class="chat-area" id="chat-area"></div>
        <!-- Live Transcript Panel -->
        <div class="transcript-panel" id="transcript-panel">
            <div id="transcript-content"></div>
        </div>

        <div class="typing-indicator" id="typing">
            <i class="fas fa-circle-notch fa-spin"></i> <span id="thinking-label">Interviewer is thinking...</span>
        </div>
        <div class="status-bar" id="status">Choose a mode to start</div>

        <div class="voice-toggle-bar">
            <div class="voice-toggle">
                <label>
                    <i class="fas fa-keyboard" id="mode-icon-text"></i>
                    <span class="toggle-switch">
                        <input type="checkbox" id="voice-toggle" onchange="toggleVoiceMode()" checked>
                        <span class="toggle-slider"></span>
                    </span>
                    <i class="fas fa-volume-up" id="mode-icon-voice"></i>
                    <span id="voice-mode-label">Voice mode</span>
                    <span class="cost-badge active" id="cost-badge">realtime AI</span>
                </label>
            </div>
            <div class="session-timer" id="session-timer">00:00</div>
        </div>

        <div class="input-area">
            <input type="text" id="text-input" placeholder="Type a message..." maxlength="2000" onkeypress="if(event.key==='Enter')sendText()">
            <button class="send-btn" onclick="sendText()" title="Send text">
                <i class="fas fa-paper-plane"></i>
            </button>
            <button class="mic-btn" id="mic-btn" onclick="toggleRecording()" title="Tap to start/stop voice recording">
                <i class="fas fa-microphone"></i>
            </button>
        </div>

        <div style="text-align:center; padding:6px; font-size:11px; color:#999; background:#fff; flex-shrink:0; border-top:1px solid #eee;" class="footer-links">
            <a href="/privacy" style="color:#888; text-decoration:none; margin:0 8px;">Privacy Policy</a>
            <span style="color:#ccc;">|</span>
            <a href="/terms" style="color:#888; text-decoration:none; margin:0 8px;">Terms of Service</a>
        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.7.4/socket.io.min.js"></script>
    <script>
        const socket = io({
            reconnection: true,
            reconnectionAttempts: 10,
            reconnectionDelay: 1000,
            reconnectionDelayMax: 5000,
            timeout: 20000
        });
        const chatArea = document.getElementById('chat-area');
        const textInput = document.getElementById('text-input');
        const micBtn = document.getElementById('mic-btn');
        const typingEl = document.getElementById('typing');
        const statusEl = document.getElementById('status');

        // ============================================================
        // CONNECTION: Reconnection & error handling
        // ============================================================
        let isConnected = false;

        socket.on('connect', () => {
            isConnected = true;
            statusEl.textContent = 'Connected ‚Äî choose a mode to start';
            statusEl.style.color = '#666';
            // Sync voice mode default with server
            socket.emit('toggle_voice_mode', { voice_mode: voiceModeOn });
        });

        socket.on('disconnect', (reason) => {
            isConnected = false;
            typingEl.style.display = 'none';
            statusEl.textContent = 'Disconnected ‚Äî reconnecting...';
            statusEl.style.color = '#dc3545';
            if (reason === 'io server disconnect') {
                socket.connect(); // Server disconnected us, reconnect manually
            }
        });

        socket.on('reconnect', (attemptNumber) => {
            isConnected = true;
            statusEl.textContent = 'Reconnected! You may need to restart your session.';
            statusEl.style.color = '#28a745';
            setTimeout(() => { statusEl.style.color = '#666'; }, 3000);
        });

        socket.on('reconnect_attempt', (attemptNumber) => {
            statusEl.textContent = `Reconnecting... (attempt ${attemptNumber})`;
        });

        socket.on('reconnect_failed', () => {
            statusEl.textContent = 'Connection lost. Please refresh the page.';
            statusEl.style.color = '#dc3545';
        });

        socket.on('connect_error', (err) => {
            statusEl.textContent = 'Connection error ‚Äî retrying...';
            statusEl.style.color = '#dc3545';
        });

        let mediaRecorder = null;
        let audioChunks = [];
        let isRecording = false;
        let currentAudio = null;  // Track currently playing audio
        let botIsPlaying = false; // True while bot audio is playing
        let isProcessing = false; // True while waiting for bot response (prevents message piling)

        // ============================================================
        // STREAMING AUDIO: Progressive chunk queue for natural playback
        // Blueprint: Streaming TTS Layer ‚Äî chunk-based streaming
        // ============================================================
        let audioChunkQueue = [];     // Queue of {audio_b64, chunk_index} waiting to play
        let isPlayingChunks = false;  // True while playing through the queue
        let streamingTextBuffer = ''; // Accumulates text chunks for display
        let streamingBubbleEl = null; // Reference to the bubble being streamed into
        let streamStartTime = null;   // For latency tracking

        function enqueueAudioChunk(audio_b64, chunkIndex) {
            audioChunkQueue.push({ audio_b64, chunkIndex });
            if (!isPlayingChunks) {
                playNextChunk();
            }
        }

        function playNextChunk() {
            if (audioChunkQueue.length === 0) {
                isPlayingChunks = false;
                botIsPlaying = false;
                // If interrupt monitor is running, let IT hand off the pre-warmed mic
                // (avoids race where both try to start listening and pre-warmed stream is wasted)
                if (autoListenEnabled && !isRecording && !interruptMonitorInterval) {
                    setTimeout(() => startListening(), 30); // Tiny echo-guard delay
                }
                return;
            }

            isPlayingChunks = true;
            botIsPlaying = true;
            const chunk = audioChunkQueue.shift();
            const audioBytes = Uint8Array.from(atob(chunk.audio_b64), c => c.charCodeAt(0));
            const blob = new Blob([audioBytes], { type: 'audio/ogg; codecs=opus' });
            const url = URL.createObjectURL(blob);

            currentAudio = new Audio(url);
            currentAudio.onended = () => {
                URL.revokeObjectURL(url);
                currentAudio = null;
                // Play next chunk in queue
                playNextChunk();
            };
            currentAudio.onerror = () => {
                URL.revokeObjectURL(url);
                currentAudio = null;
                playNextChunk();
            };
            currentAudio.play().catch(e => {
                console.log('Chunk autoplay blocked:', e);
                currentAudio = null;
                playNextChunk();
            });
        }

        function flushAudioQueue() {
            audioChunkQueue = [];
            isPlayingChunks = false;
        }

        // ============================================================
        // COST OPTIMIZATION: Voice mode state
        // ============================================================
        let voiceModeOn = true;    // Voice-first by default
        let sessionStartTime = Date.now();
        let sessionTimerInterval = null;
        let pendingTtsMessages = {};  // {msg_id: element} for on-demand TTS
        let currentMode = null;        // Track active mode: 'interview', 'helpdesk', 'language'
        let currentLanguage = 'en';   // Track selected language for language mode
        let waitingForBotAudio = false; // True between text_chunk and audio_chunk (pre-warm phase)

        // CV & Job Profile context (uploaded text)
        let uploadedCV = '';
        let uploadedJobProfile = '';

        // Role-specific labels
        const ROLE_LABELS = {
            interview: { thinking: 'Interviewer is thinking...', name: 'Charlotte ‚Äî Interviewer', icon: 'fa-user-tie' },
            helpdesk: { thinking: 'IT Helpdesk is thinking...', name: 'Sam ‚Äî IT Support', icon: 'fa-headset' },
            language: { thinking: 'Language Coach is thinking...', name: 'Language Coach', icon: 'fa-language' },
        };

        // SECURITY: Escape HTML to prevent XSS
        function escapeHTML(str) {
            const div = document.createElement('div');
            div.appendChild(document.createTextNode(str));
            return div.innerHTML;
        }

        // --- Session Timer ---
        function startSessionTimer() {
            sessionStartTime = Date.now();
            if (sessionTimerInterval) clearInterval(sessionTimerInterval);
            sessionTimerInterval = setInterval(() => {
                const elapsed = Math.floor((Date.now() - sessionStartTime) / 1000);
                const mins = String(Math.floor(elapsed / 60)).padStart(2, '0');
                const secs = String(elapsed % 60).padStart(2, '0');
                document.getElementById('session-timer').textContent = `${mins}:${secs}`;
            }, 1000);
        }

        // --- Voice Mode Toggle ---
        function toggleVoiceMode() {
            const toggle = document.getElementById('voice-toggle');
            voiceModeOn = toggle.checked;
            socket.emit('toggle_voice_mode', { voice_mode: voiceModeOn });

            const label = document.getElementById('voice-mode-label');
            const badge = document.getElementById('cost-badge');
            if (voiceModeOn) {
                label.textContent = 'Voice mode';
                badge.textContent = 'realtime AI';
                badge.className = 'cost-badge active';
                // If a mode is active, connect Realtime API
                if (currentMode) {
                    connectRealtime(currentMode, currentLanguage, uploadedCV, uploadedJobProfile);
                }
            } else {
                label.textContent = 'Text mode';
                badge.textContent = '$$$ saving';
                badge.className = 'cost-badge saving';
                // Disconnect Realtime session
                disconnectRealtime();
            }
        }

        // ============================================================
        // OPENAI REALTIME API: WebRTC for ChatGPT-level voice quality
        // Audio goes directly browser ‚Üî OpenAI (~300ms latency)
        // Server VAD, auto-interruption, natural turn-taking built in
        // ============================================================
        let realtimePc = null;          // RTCPeerConnection
        let realtimeDc = null;          // DataChannel for events
        let realtimeAudio = null;       // Audio element for model output
        let realtimeMicStream = null;   // Mic stream for WebRTC
        let realtimeConnected = false;  // True when data channel is open
        let realtimeSessionId = null;
        let realtimeTextBuffer = '';    // Accumulates model text response
        let realtimeBubbleEl = null;    // Current streaming bot bubble
        let realtimeMuted = false;      // Mic mute state (user-initiated)
        let realtimeModelSpeaking = false; // True while model is outputting audio

        async function connectRealtime(mode, language, cvText, jobText) {
            // Disconnect any existing session
            disconnectRealtime();

            // Kill ALL legacy voice components ‚Äî they conflict with Realtime API
            stopBrowserSTT();
            stopInterruptionMonitor();
            stopVAD();
            if (isRecording && mediaRecorder) {
                try { mediaRecorder.stop(); } catch(e) {}
                isRecording = false;
            }
            releaseMicStream();
            stopCurrentAudio();
            flushAudioQueue();

            try {
                statusEl.textContent = 'üîÑ Connecting to voice AI...';

                // 1. Get ephemeral token from our server
                // All config (instructions, voice, VAD) is baked into the token
                // POST with CV/Job context if available
                const tokenBody = {
                    mode: mode,
                    language: language || 'en',
                };
                if (cvText) tokenBody.cv_text = cvText;
                if (jobText) tokenBody.job_profile_text = jobText;

                const tokenRes = await fetch('/api/realtime/token', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(tokenBody),
                });
                if (!tokenRes.ok) {
                    const errText = await tokenRes.text();
                    throw new Error(`Token failed: ${tokenRes.status} ${errText}`);
                }
                const tokenData = await tokenRes.json();
                const EPHEMERAL_KEY = tokenData.client_secret?.value || tokenData.value;
                if (!EPHEMERAL_KEY) {
                    console.error('[Realtime] Token response:', tokenData);
                    throw new Error('No ephemeral key received');
                }
                console.log('[Realtime] Got ephemeral token');

                // 2. Create RTCPeerConnection
                realtimePc = new RTCPeerConnection();

                // 3. Set up audio playback for model responses
                realtimeAudio = document.createElement('audio');
                realtimeAudio.autoplay = true;
                realtimePc.ontrack = (e) => (realtimeAudio.srcObject = e.streams[0]);

                // 4. Add microphone audio track
                // Use simple constraints like OpenAI's reference implementation
                realtimeMicStream = await navigator.mediaDevices.getUserMedia({
                    audio: true,
                });
                const micTrack = realtimeMicStream.getTracks()[0];
                console.log('[Realtime] Mic track:', micTrack.label, 'enabled:', micTrack.enabled, 'muted:', micTrack.muted);
                realtimePc.addTrack(micTrack);

                // 5. Create data channel for Realtime events
                realtimeDc = realtimePc.createDataChannel('oai-events');
                realtimeDc.onopen = () => {
                    console.log('[Realtime] Data channel open ‚Äî session active');
                    realtimeConnected = true;
                    micBtn.classList.add('recording');
                    statusEl.textContent = 'üé§ Connected ‚Äî speak naturally!';

                    // Log connection details for debugging
                    if (realtimePc) {
                        const senders = realtimePc.getSenders();
                        console.log('[Realtime] Senders:', senders.length);
                        senders.forEach((s, i) => {
                            console.log(`[Realtime] Sender ${i}: kind=${s.track?.kind} enabled=${s.track?.enabled} readyState=${s.track?.readyState}`);
                        });
                        const receivers = realtimePc.getReceivers();
                        console.log('[Realtime] Receivers:', receivers.length);
                        receivers.forEach((r, i) => {
                            console.log(`[Realtime] Receiver ${i}: kind=${r.track?.kind}`);
                        });
                    }

                    // Trigger initial greeting from the AI
                    realtimeDc.send(JSON.stringify({
                        type: 'response.create',
                    }));
                };
                realtimeDc.onclose = () => {
                    console.log('[Realtime] Data channel closed');
                    realtimeConnected = false;
                    micBtn.classList.remove('recording');
                    statusEl.textContent = 'Voice session ended';
                };
                realtimeDc.onmessage = (event) => {
                    handleRealtimeEvent(JSON.parse(event.data));
                };

                // 6. Handle ICE connection state
                realtimePc.oniceconnectionstatechange = () => {
                    const state = realtimePc.iceConnectionState;
                    console.log('[Realtime] ICE state:', state);
                    if (state === 'failed' || state === 'disconnected') {
                        statusEl.textContent = 'Voice connection lost ‚Äî reconnecting...';
                        setTimeout(() => {
                            if (!realtimeConnected && currentMode) {
                                connectRealtime(currentMode, currentLanguage, uploadedCV, uploadedJobProfile);
                            }
                        }, 2000);
                    }
                };

                // 7. Create SDP offer
                const offer = await realtimePc.createOffer();
                await realtimePc.setLocalDescription(offer);

                // 8. Connect DIRECTLY to OpenAI ‚Äî matches reference implementation
                // Include ?model= even with ephemeral tokens (reference does this)
                const model = 'gpt-realtime';
                const sdpRes = await fetch(
                    `https://api.openai.com/v1/realtime/calls?model=${model}`,
                    {
                        method: 'POST',
                        body: offer.sdp,
                        headers: {
                            'Authorization': `Bearer ${EPHEMERAL_KEY}`,
                            'Content-Type': 'application/sdp',
                        },
                    }
                );

                if (!sdpRes.ok) {
                    const errText = await sdpRes.text();
                    console.error('[Realtime] SDP response:', sdpRes.status, errText);
                    throw new Error(`SDP exchange failed: ${sdpRes.status} ${errText}`);
                }

                const answerSdp = await sdpRes.text();
                console.log('[Realtime] Got SDP answer, length:', answerSdp.length);
                await realtimePc.setRemoteDescription({
                    type: 'answer',
                    sdp: answerSdp,
                });

                console.log('[Realtime] Connected via ephemeral token (direct to OpenAI)');

            } catch (err) {
                console.error('[Realtime] Connection error:', err);
                statusEl.textContent = 'Voice connection failed ‚Äî using fallback mode';
                disconnectRealtime();
                // Fall back to old Socket.IO pipeline
                fallbackToLegacyVoice(mode, language);
            }
        }

        function disconnectRealtime() {
            if (realtimeDc) {
                try { realtimeDc.close(); } catch(e) {}
                realtimeDc = null;
            }
            if (realtimePc) {
                realtimePc.getSenders().forEach(sender => {
                    if (sender.track) sender.track.stop();
                });
                try { realtimePc.close(); } catch(e) {}
                realtimePc = null;
            }
            if (realtimeMicStream) {
                realtimeMicStream.getTracks().forEach(t => t.stop());
                realtimeMicStream = null;
            }
            if (realtimeAudio) {
                realtimeAudio.pause();
                realtimeAudio.srcObject = null;
                realtimeAudio = null;
            }
            realtimeConnected = false;
            realtimeSessionId = null;
            realtimeTextBuffer = '';
            realtimeBubbleEl = null;
            realtimeMuted = false;
            realtimeModelSpeaking = false;
            micBtn.classList.remove('recording');
        }

        function fallbackToLegacyVoice(mode, language) {
            // Use the old Socket.IO streaming pipeline as fallback
            socket.emit('toggle_voice_mode', { voice_mode: true });
            if (mode === 'interview') {
                showThinking();
                socket.emit('start_interview');
            } else if (mode === 'helpdesk') {
                showThinking();
                socket.emit('start_helpdesk');
            } else if (mode === 'language') {
                showThinking();
                socket.emit('start_language_test', { language: language });
            }
        }

        // --- Interrupt support: cancel in-flight response ---
        function interruptModel() {
            if (!realtimeConnected || !realtimeModelSpeaking) return;
            realtimeModelSpeaking = false;
            if (realtimeDc && realtimeDc.readyState === 'open') {
                // For WebRTC: use output_audio_buffer.clear to stop playback
                // and truncate conversation (recommended by OpenAI docs)
                realtimeDc.send(JSON.stringify({ type: 'response.cancel' }));
                realtimeDc.send(JSON.stringify({ type: 'output_audio_buffer.clear' }));
            }
            statusEl.textContent = 'üé§ Go ahead...';
            console.log('[Realtime] User interrupted model');
        }

        // Keyboard interrupt: Space or Enter while model is speaking
        document.addEventListener('keydown', (e) => {
            if (!realtimeConnected || !realtimeModelSpeaking) return;
            if (e.code === 'Space' || e.code === 'Enter') {
                // Don't trigger if typing in the text input
                if (e.target.tagName === 'INPUT' || e.target.tagName === 'TEXTAREA') return;
                e.preventDefault();
                interruptModel();
            }
        });

        function toggleRealtimeMute() {
            if (!realtimeMicStream) return;
            // If model is speaking, treat click as interrupt
            if (realtimeModelSpeaking) {
                interruptModel();
                return;
            }
            realtimeMuted = !realtimeMuted;
            realtimeMicStream.getTracks().forEach(track => {
                track.enabled = !realtimeMuted;
            });
            if (realtimeMuted) {
                micBtn.classList.remove('recording');
                statusEl.textContent = 'üîá Muted ‚Äî click mic to unmute';
            } else {
                micBtn.classList.add('recording');
                statusEl.textContent = 'üé§ Listening ‚Äî speak naturally';
            }
        }

        function handleRealtimeEvent(event) {
            switch (event.type) {
                case 'session.created':
                    realtimeSessionId = event.session?.id || event.session;
                    console.log('[Realtime] Session created:', realtimeSessionId);
                    const sess = event.session || {};
                    console.log('[Realtime] Model:', sess.model);
                    console.log('[Realtime] Voice:', JSON.stringify(sess.audio?.output));
                    console.log('[Realtime] VAD:', JSON.stringify(sess.audio?.input?.turn_detection));
                    console.log('[Realtime] Transcription:', JSON.stringify(sess.audio?.input?.transcription));
                    console.log('[Realtime] Modalities:', sess.output_modalities);
                    console.log('[Realtime] Instructions (first 100):', (sess.instructions || '').substring(0, 100));
                    break;

                case 'session.updated':
                    console.log('[Realtime] Session updated ‚Äî semantic_vad active');
                    break;

                case 'input_audio_buffer.speech_started':
                    // User started speaking ‚Äî semantic_vad detected meaningful speech
                    console.log('[Realtime] >>> Speech STARTED ‚Äî mic audio IS flowing to OpenAI');
                    statusEl.textContent = 'üé§ Listening...';
                    typingEl.style.display = 'none';
                    setAvatarSpeaking(false);
                    // With interrupt_response: true, the server automatically
                    // cancels any in-progress model response. We just update UI.
                    if (realtimeModelSpeaking) {
                        realtimeModelSpeaking = false;
                        // Finalize partial bubble
                        if (realtimeBubbleEl && realtimeTextBuffer.trim()) {
                            realtimeBubbleEl.textContent = realtimeTextBuffer + ' [interrupted]';
                            realtimeBubbleEl.classList.remove('streaming-bubble');
                        }
                        realtimeTextBuffer = '';
                        realtimeBubbleEl = null;
                        console.log('[Realtime] Cancelled stale response ‚Äî user is speaking');
                    }
                    break;

                case 'input_audio_buffer.speech_stopped':
                    console.log('[Realtime] >>> Speech STOPPED ‚Äî processing user input');
                    statusEl.textContent = '‚ö° Processing...';
                    typingEl.style.display = 'block';
                    break;

                case 'conversation.item.input_audio_transcription.completed':
                    // User's speech transcribed ‚Äî show in chat
                    console.log('[Realtime] >>> User transcript:', event.transcript);
                    if (event.transcript && event.transcript.trim()) {
                        addUserMessage(event.transcript.trim());
                        addTranscriptLine('user', event.transcript.trim());
                        // Log to backend for conversation history
                        socket.emit('realtime_log', {
                            role: 'user',
                            text: event.transcript.trim(),
                            mode: currentMode,
                        });
                    }
                    break;

                case 'response.audio_transcript.delta':
                    // Model's text response streaming in real-time
                    typingEl.style.display = 'none';
                    const delta = event.delta;
                    realtimeTextBuffer += delta;

                    // Track model speaking state
                    if (!realtimeModelSpeaking) {
                        realtimeModelSpeaking = true;
                        statusEl.textContent = 'üîä Speaking... (just speak to interrupt)';
                        setAvatarSpeaking(true);
                    }

                    if (!realtimeBubbleEl) {
                        const div = document.createElement('div');
                        div.className = 'message bot';
                        const icon = getBotIcon();
                        div.innerHTML = `
                            <div class="avatar"><i class="fas ${icon}"></i></div>
                            <div>
                                <div class="bubble streaming-bubble">${escapeHTML(realtimeTextBuffer)}</div>
                            </div>`;
                        chatArea.appendChild(div);
                        realtimeBubbleEl = div.querySelector('.streaming-bubble');
                    } else {
                        realtimeBubbleEl.textContent = realtimeTextBuffer;
                    }
                    chatArea.scrollTop = chatArea.scrollHeight;
                    break;

                case 'response.audio_transcript.done':
                    // Model finished this transcript segment
                    if (realtimeBubbleEl) {
                        realtimeBubbleEl.classList.remove('streaming-bubble');
                    }
                    // Add to transcript panel
                    if (realtimeTextBuffer.trim()) {
                        addTranscriptLine('bot', realtimeTextBuffer.trim());
                    }
                    // Log to backend
                    if (realtimeTextBuffer.trim()) {
                        socket.emit('realtime_log', {
                            role: 'assistant',
                            text: realtimeTextBuffer.trim(),
                            mode: currentMode,
                        });
                    }
                    realtimeTextBuffer = '';
                    realtimeBubbleEl = null;
                    break;

                case 'response.done':
                    typingEl.style.display = 'none';
                    realtimeModelSpeaking = false;
                    setAvatarSpeaking(false);
                    statusEl.textContent = 'üé§ Ready ‚Äî speak anytime';
                    break;

                case 'response.cancelled':
                    // User interrupted ‚Äî OpenAI auto-cancelled the response
                    typingEl.style.display = 'none';
                    statusEl.textContent = 'üé§ Go ahead...';
                    // Finalize any partial bubble
                    if (realtimeBubbleEl) {
                        if (realtimeTextBuffer.trim()) {
                            realtimeBubbleEl.textContent = realtimeTextBuffer + ' [interrupted]';
                            realtimeBubbleEl.classList.remove('streaming-bubble');
                            socket.emit('realtime_log', {
                                role: 'assistant',
                                text: realtimeTextBuffer.trim() + ' [interrupted]',
                                mode: currentMode,
                            });
                        } else {
                            realtimeBubbleEl.parentElement.parentElement.remove();
                        }
                    }
                    realtimeTextBuffer = '';
                    realtimeBubbleEl = null;
                    realtimeModelSpeaking = false;
                    break;

                case 'error':
                    console.error('[Realtime] Error:', event.error);
                    if (event.error && event.error.message) {
                        statusEl.textContent = '‚ö†Ô∏è ' + event.error.message;
                    } else {
                        statusEl.textContent = '‚ö†Ô∏è Voice error ‚Äî try speaking again';
                    }
                    break;

                case 'rate_limits.updated':
                    // Info only ‚Äî log for monitoring
                    console.log('[Realtime] Rate limits:', event.rate_limits);
                    break;

                default:
                    // Log all events for debugging (except high-frequency audio)
                    if (event.type && !event.type.startsWith('response.audio.delta')) {
                        console.log('[Realtime] Event:', event.type, JSON.stringify(event).substring(0, 200));
                    }
            }
        }

        // ============================================================
        // BROWSER SPEECH RECOGNITION: Real-time STT (instant, no upload)
        // Falls back to MediaRecorder+Whisper for non-English or unsupported browsers
        // ============================================================
        let speechRecognition = null;
        let useBrowserSTT = false;
        let browserSTTResult = '';
        let browserSTTFinal = false;

        // Check if browser SpeechRecognition is available
        const SpeechRecognitionAPI = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (SpeechRecognitionAPI) {
            useBrowserSTT = true;
            console.log('[STT] Browser SpeechRecognition available ‚Äî using for instant transcription');
        } else {
            console.log('[STT] Browser SpeechRecognition not available ‚Äî using Whisper fallback');
        }

        function startBrowserSTT() {
            // Don't run legacy STT when Realtime API handles transcription
            if (realtimeConnected) return false;
            if (!SpeechRecognitionAPI) return false;
            // Only use browser STT for English (other languages use Whisper for accuracy)
            if (currentMode === 'language') return false;

            try {
                speechRecognition = new SpeechRecognitionAPI();
                speechRecognition.continuous = true;
                speechRecognition.interimResults = true;
                speechRecognition.lang = 'en-US';
                speechRecognition.maxAlternatives = 1;
                browserSTTResult = '';
                browserSTTFinal = false;

                speechRecognition.onresult = (event) => {
                    let interim = '';
                    let finalTranscript = '';
                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        const transcript = event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            finalTranscript += transcript;
                        } else {
                            interim += transcript;
                        }
                    }
                    if (finalTranscript) {
                        browserSTTResult += (browserSTTResult ? ' ' : '') + finalTranscript.trim();
                    }
                    // Show live transcription in status
                    const display = browserSTTResult + (interim ? ' ' + interim : '');
                    if (display.trim()) {
                        statusEl.textContent = 'üé§ ' + display.trim().substring(0, 80) + (display.length > 80 ? '...' : '');
                    }
                };

                speechRecognition.onerror = (event) => {
                    if (event.error !== 'no-speech' && event.error !== 'aborted') {
                        console.log('[STT] Speech recognition error:', event.error);
                    }
                };

                speechRecognition.onend = () => {
                    // Browser auto-stops recognition after silence timeout
                    // Restart if we're still actively recording to keep capturing
                    if (isRecording && !botIsPlaying) {
                        try {
                            speechRecognition.start();
                            console.log('[STT] Restarted browser recognition (auto-ended)');
                        } catch(e) {
                            console.log('[STT] Could not restart:', e.message);
                        }
                    }
                };

                speechRecognition.start();
                return true;
            } catch (e) {
                console.log('[STT] Failed to start browser speech recognition:', e);
                return false;
            }
        }

        function stopBrowserSTT() {
            if (speechRecognition) {
                try { speechRecognition.stop(); } catch(e) {}
                speechRecognition = null;
            }
        }

        function getBrowserSTTText() {
            const text = browserSTTResult.trim();
            browserSTTResult = '';
            return text;
        }

        // --- VAD (Voice Activity Detection) ---
        let audioContext = null;
        let analyser = null;
        let micStream = null;
        let vadInterval = null;
        let silenceStart = null;
        let speechDetected = false;
        let autoListenEnabled = true;   // auto-listen enabled by default
        let wasInterrupted = false; // Track if user interrupted bot
        const SILENCE_THRESHOLD = 12;   // RMS below this = silence (sensitive for reliable detection)
        const SILENCE_DURATION = 1500;  // 1.5s silence ‚Äî natural pacing (1s was cutting people off mid-thought)
        const VAD_CHECK_INTERVAL = 40;  // ms between VAD checks
        const INTERRUPT_THRESHOLD = 16; // RMS threshold for interrupt detection during bot playback
        const INTERRUPT_SPEECH_MIN_MS = 150; // 150ms to count as real interruption (near-instant)
        let speechStartTime = null;     // when current speech burst started
        let interruptionDetectedAt = null; // timestamp when interruption was detected

        // --- Lightweight interruption monitor (no recording, just volume check) ---
        let interruptMonitorStream = null;
        let interruptMonitorCtx = null;
        let interruptMonitorAnalyser = null;
        let interruptMonitorInterval = null;
        let interruptSpeechStart = null;

        // Pre-warm: acquire mic stream during bot playback so it's ready for instant handoff
        let prewarmedMicStream = null;

        async function startInterruptionMonitor() {
            // Don't run legacy interruption monitor when Realtime API handles everything
            if (realtimeConnected) return;
            stopInterruptionMonitor();
            try {
                // Acquire mic stream (will be handed off to startListening on interrupt)
                interruptMonitorStream = await navigator.mediaDevices.getUserMedia({
                    audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: true }
                });
                prewarmedMicStream = interruptMonitorStream; // Save for handoff
                interruptMonitorCtx = new (window.AudioContext || window.webkitAudioContext)();
                const source = interruptMonitorCtx.createMediaStreamSource(interruptMonitorStream);
                interruptMonitorAnalyser = interruptMonitorCtx.createAnalyser();
                interruptMonitorAnalyser.fftSize = 2048; // Reliable RMS ‚Äî 42ms window (512 was too noisy)
                source.connect(interruptMonitorAnalyser);
                interruptSpeechStart = null;

                interruptMonitorInterval = setInterval(() => {
                    if (!botIsPlaying && !waitingForBotAudio) {
                        // Bot finished all audio ‚Äî hand off mic to real recording
                        const stream = interruptMonitorStream;
                        // Clear references but DON'T stop the stream ‚Äî hand it off
                        clearInterval(interruptMonitorInterval);
                        interruptMonitorInterval = null;
                        if (interruptMonitorCtx) {
                            interruptMonitorCtx.close().catch(() => {});
                            interruptMonitorCtx = null;
                            interruptMonitorAnalyser = null;
                        }
                        interruptMonitorStream = null;
                        interruptSpeechStart = null;
                        if (autoListenEnabled && !isRecording) {
                            // Hand off the pre-warmed stream (zero mic startup delay)
                            startListening(stream);
                        } else if (stream) {
                            stream.getTracks().forEach(t => t.stop());
                        }
                        prewarmedMicStream = null;
                        return;
                    }
                    const rms = getRMS(interruptMonitorAnalyser);
                    if (rms > INTERRUPT_THRESHOLD) {
                        if (!interruptSpeechStart) interruptSpeechStart = Date.now();
                        // User has been speaking long enough ‚Äî real interruption
                        if (Date.now() - interruptSpeechStart > INTERRUPT_SPEECH_MIN_MS) {
                            console.log('[Interrupt] User interrupted bot, RMS:', rms.toFixed(1));
                            // Blueprint: Interruption flow ‚Äî cancel active LLM + TTS + flush buffer
                            socket.emit('cancel_stream');  // Signal backend to cancel generation
                            stopCurrentAudio();

                            // Grab the pre-warmed stream before clearing monitor state
                            const stream = interruptMonitorStream;
                            clearInterval(interruptMonitorInterval);
                            interruptMonitorInterval = null;
                            if (interruptMonitorCtx) {
                                interruptMonitorCtx.close().catch(() => {});
                                interruptMonitorCtx = null;
                                interruptMonitorAnalyser = null;
                            }
                            interruptMonitorStream = null;
                            interruptSpeechStart = null;

                            wasInterrupted = true;
                            statusEl.textContent = 'üé§ Go ahead, I\'m listening...';
                            // Hand off pre-warmed stream ‚Äî ZERO getUserMedia delay
                            startListening(stream);
                            prewarmedMicStream = null;
                        }
                    } else {
                        interruptSpeechStart = null;
                    }
                }, VAD_CHECK_INTERVAL);
            } catch (e) {
                console.log('Interrupt monitor mic error:', e);
            }
        }

        function stopInterruptionMonitor() {
            if (interruptMonitorInterval) {
                clearInterval(interruptMonitorInterval);
                interruptMonitorInterval = null;
            }
            if (interruptMonitorStream) {
                interruptMonitorStream.getTracks().forEach(t => t.stop());
                interruptMonitorStream = null;
            }
            prewarmedMicStream = null;
            if (interruptMonitorCtx) {
                interruptMonitorCtx.close().catch(() => {});
                interruptMonitorCtx = null;
                interruptMonitorAnalyser = null;
            }
            interruptSpeechStart = null;
        }

        function getRMS(analyserNode) {
            if (!analyserNode) return 0;
            const data = new Uint8Array(analyserNode.fftSize);
            analyserNode.getByteTimeDomainData(data);
            let sum = 0;
            for (let i = 0; i < data.length; i++) {
                const val = (data[i] - 128);
                sum += val * val;
            }
            return Math.sqrt(sum / data.length);
        }

        function startVAD() {
            if (!analyser) return;
            silenceStart = null;
            speechDetected = false;
            speechStartTime = null;
            interruptionDetectedAt = null;
            vadInterval = setInterval(() => {
                // NEVER do VAD speech detection while bot is playing
                // (interruption is handled by the separate interruptionMonitor)
                if (botIsPlaying) return;

                const rms = getRMS(analyser);
                if (rms > SILENCE_THRESHOLD) {
                    // Voice detected
                    if (!speechDetected) {
                        speechStartTime = Date.now();
                    }
                    speechDetected = true;
                    silenceStart = null;
                    // Don't overwrite live transcription status if browser STT is showing it
                    if (!useBrowserSTT || !browserSTTResult) {
                        statusEl.textContent = 'üé§ Listening...';
                    }
                } else if (speechDetected) {
                    // Was speaking, now silent
                    if (!silenceStart) {
                        silenceStart = Date.now();
                    } else if (Date.now() - silenceStart > SILENCE_DURATION) {
                        stopRecordingAndSend();
                    }
                }
            }, VAD_CHECK_INTERVAL);
        }

        function stopVAD() {
            if (vadInterval) {
                clearInterval(vadInterval);
                vadInterval = null;
            }
        }

        // Stop any currently playing bot audio + flush streaming queue
        function stopCurrentAudio() {
            if (currentAudio) {
                try {
                    currentAudio.onended = null;
                    currentAudio.onerror = null;
                    currentAudio.pause();
                    currentAudio.removeAttribute('src');
                    currentAudio.load();  // Force release audio resource
                } catch(e) {}
                currentAudio = null;
            }
            // Flush streaming audio queue (blueprint: flush playback buffer)
            flushAudioQueue();
            botIsPlaying = false;
            waitingForBotAudio = false;
            // Reset streaming display state
            streamingTextBuffer = '';
            streamingBubbleEl = null;
            streamStartTime = null;
        }

        // Release microphone stream fully
        function releaseMicStream() {
            if (micStream) {
                micStream.getTracks().forEach(t => t.stop());
                micStream = null;
            }
            if (audioContext) {
                audioContext.close().catch(() => {});
                audioContext = null;
                analyser = null;
            }
        }

        // --- Socket events ---

        // ============================================================
        // STREAMING: Handle progressive text + audio chunks
        // Blueprint: Streaming TTS Layer + Progressive Playback Buffer
        // ============================================================
        socket.on('text_chunk', (data) => {
            // Don't process legacy streaming when Realtime API is active
            if (realtimeConnected) return;
            typingEl.style.display = 'none';

            if (!streamStartTime) streamStartTime = Date.now();

            // Pre-warm interrupt monitor on first text chunk (before audio arrives)
            // This gives ~500-1000ms head start for mic initialization
            if (data.chunk_index === 0 && autoListenEnabled && !interruptMonitorInterval) {
                waitingForBotAudio = true;
                startInterruptionMonitor();
            }

            // Progressive text display: append to existing bot bubble or create new one
            const chunkText = data.text;
            streamingTextBuffer += (streamingTextBuffer ? ' ' : '') + chunkText;

            if (!streamingBubbleEl) {
                // Create the bot message container on first chunk
                const div = document.createElement('div');
                div.className = 'message bot';
                div.id = 'streaming-msg';
                const safeText = escapeHTML(streamingTextBuffer);
                const icon = getBotIcon();
                div.innerHTML = `
                    <div class="avatar"><i class="fas ${icon}"></i></div>
                    <div>
                        <div class="bubble streaming-bubble">${safeText}</div>
                    </div>`;
                chatArea.appendChild(div);
                streamingBubbleEl = div.querySelector('.streaming-bubble');
            } else {
                // Update existing bubble with accumulated text
                streamingBubbleEl.textContent = streamingTextBuffer;
            }
            chatArea.scrollTop = chatArea.scrollHeight;
        });

        socket.on('audio_chunk', (data) => {
            // Don't process legacy audio when Realtime API handles audio via WebRTC
            if (realtimeConnected) return;
            typingEl.style.display = 'none';

            if (!streamStartTime) streamStartTime = Date.now();

            // Track time-to-first-audio
            if (data.chunk_index === 0) {
                const latency = Date.now() - (streamStartTime || Date.now());
                console.log(`[Streaming] First audio chunk latency: ${latency}ms`);
            }

            // Queue audio chunk for sequential playback
            enqueueAudioChunk(data.audio, data.chunk_index);

            // Clear pre-warm flag ‚Äî audio is now being played
            waitingForBotAudio = false;

            // Start interruption monitor if not already running (fallback)
            if (data.chunk_index === 0 && autoListenEnabled && !interruptMonitorInterval) {
                startInterruptionMonitor();
            }
        });

        socket.on('stream_complete', (data) => {
            // Don't process legacy streaming when Realtime API is active
            if (realtimeConnected) return;
            // Streaming is done ‚Äî finalize the message
            isProcessing = false;
            const totalTime = Date.now() - (streamStartTime || Date.now());
            console.log(`[Streaming] Complete: ${data.total_chunks} chunks, ${totalTime}ms`);

            // Add audio player to the completed message if we have audio
            if (streamingBubbleEl && streamingBubbleEl.parentElement) {
                // Message is already displayed progressively ‚Äî nothing more needed
            }

            // Reset streaming state
            streamingTextBuffer = '';
            streamingBubbleEl = null;
            streamStartTime = null;
        });

        // Legacy: Full audio response (used for non-streaming fallback)
        socket.on('audio_response', (data) => {
            if (realtimeConnected) return;
            typingEl.style.display = 'none';
            // Stop any previous audio first
            stopCurrentAudio();
            flushAudioQueue();

            // Play audio
            const audioBytes = Uint8Array.from(atob(data.audio), c => c.charCodeAt(0));
            const blob = new Blob([audioBytes], { type: 'audio/ogg; codecs=opus' });
            const url = URL.createObjectURL(blob);

            // Add bot message with audio player
            addBotMessage(data.text, url);

            // Auto-play and track
            currentAudio = new Audio(url);
            botIsPlaying = true;
            currentAudio.onended = () => {
                currentAudio = null;
                botIsPlaying = false;
                // Auto-listen after bot finishes speaking (zero delay)
                if (autoListenEnabled && !isRecording) {
                    startListening();
                }
            };
            currentAudio.play().catch(e => {
                console.log('Autoplay blocked:', e);
                botIsPlaying = false;
                // Auto-listen since playback failed
                if (autoListenEnabled && !isRecording) {
                    startListening();
                }
            });

            // Start lightweight interruption monitor (NOT recording ‚Äî just volume detection)
            if (autoListenEnabled) {
                startInterruptionMonitor();
            }
        });

        // COST OPTIMIZATION: Text-only response with on-demand play button
        socket.on('text_response', (data) => {
            typingEl.style.display = 'none';
            isProcessing = false;
            addBotMessageWithPlayBtn(data.text, data.msg_id);
        });

        // COST OPTIMIZATION: On-demand TTS audio received
        socket.on('tts_audio', (data) => {
            const msgId = data.msg_id;
            const audioBytes = Uint8Array.from(atob(data.audio), c => c.charCodeAt(0));
            const blob = new Blob([audioBytes], { type: 'audio/ogg; codecs=opus' });
            const url = URL.createObjectURL(blob);

            // Update the play button to show audio player
            const container = pendingTtsMessages[msgId];
            if (container) {
                const audioDiv = container.querySelector('.tts-play-area');
                if (audioDiv) {
                    audioDiv.innerHTML = `<audio controls autoplay src="${url}" style="height:32px;width:100%;margin-top:5px;"></audio>`;
                }
                delete pendingTtsMessages[msgId];
            }

            // Also play it
            stopCurrentAudio();
            currentAudio = new Audio(url);
            botIsPlaying = true;
            currentAudio.onended = () => {
                currentAudio = null;
                botIsPlaying = false;
            };
            currentAudio.play().catch(() => { botIsPlaying = false; });
        });

        socket.on('voice_mode_changed', (data) => {
            statusEl.textContent = data.message;
        });

        socket.on('status', (data) => {
            if (realtimeConnected) return;
            typingEl.style.display = 'none';
            isProcessing = false;
            statusEl.textContent = data.message;
            // Auto-restart listening after error/status if auto-listen is active
            if (autoListenEnabled && !isRecording && !botIsPlaying) {
                setTimeout(() => startListening(), 500);
            }
        });

        // Show user's transcribed text in chat (replaces voice message indicator)
        socket.on('user_transcription', (data) => {
            // Don't show legacy transcription when Realtime API handles it
            if (realtimeConnected) return;
            if (data.text) {
                addUserMessage(data.text);
            }
        });

        // --- UI functions ---
        function getBotIcon() {
            return (ROLE_LABELS[currentMode] || ROLE_LABELS.interview).icon;
        }

        function addBotMessage(text, audioUrl) {
            const div = document.createElement('div');
            div.className = 'message bot';
            // SECURITY: Escape text to prevent XSS
            const safeText = escapeHTML(text);
            const icon = getBotIcon();
            div.innerHTML = `
                <div class="avatar"><i class="fas ${icon}"></i></div>
                <div>
                    <div class="bubble">${safeText}</div>
                    ${audioUrl ? `<div class="audio-player"><audio controls src="${audioUrl}"></audio></div>` : ''}
                </div>`;
            chatArea.appendChild(div);
            chatArea.scrollTop = chatArea.scrollHeight;
            addTranscriptLine('bot', text);
        }

        function addUserMessage(text) {
            const div = document.createElement('div');
            div.className = 'message user';
            // SECURITY: Escape text to prevent XSS
            const safeText = escapeHTML(text);
            div.innerHTML = `
                <div class="bubble">${safeText}</div>
                <div class="avatar"><i class="fas fa-user"></i></div>`;
            chatArea.appendChild(div);
            chatArea.scrollTop = chatArea.scrollHeight;
            addTranscriptLine('user', text);
        }

        // COST OPTIMIZATION: Bot message with on-demand play button (no auto TTS)
        function addBotMessageWithPlayBtn(text, msgId) {
            const div = document.createElement('div');
            div.className = 'message bot';
            const safeText = escapeHTML(text);
            const encodedText = btoa(encodeURIComponent(text));
            const icon = getBotIcon();
            div.innerHTML = `
                <div class="avatar"><i class="fas ${icon}"></i></div>
                <div>
                    <div class="bubble">${safeText}</div>
                    <div class="tts-play-area">
                        <button class="play-tts-btn" data-msgid="${msgId}" data-encoded="${encodedText}">
                            <i class="fas fa-volume-up"></i> Play audio
                        </button>
                    </div>
                </div>`;
            // Attach click handler safely (avoids inline JS injection)
            const playBtn = div.querySelector('.play-tts-btn');
            playBtn.addEventListener('click', function() {
                requestTTS(this, msgId, encodedText);
            });
            chatArea.appendChild(div);
            chatArea.scrollTop = chatArea.scrollHeight;
            // Store reference for when TTS comes back
            pendingTtsMessages[msgId] = div;
        }

        // Request on-demand TTS for a specific message
        function requestTTS(btn, msgId, encodedText) {
            const text = decodeURIComponent(atob(encodedText));
            btn.disabled = true;
            btn.innerHTML = '<i class="fas fa-spinner fa-spin"></i> Generating...';
            socket.emit('request_tts', { text: text, msg_id: msgId });
        }

        function showThinking() {
            // Set role-specific thinking label
            const label = document.getElementById('thinking-label');
            const role = ROLE_LABELS[currentMode] || ROLE_LABELS.interview;
            label.textContent = role.thinking;
            typingEl.style.display = 'block';
            chatArea.scrollTop = chatArea.scrollHeight;
            isProcessing = true;
        }

        // ============================================================
        // FILE UPLOAD: CV & Job Profile for interview context
        // ============================================================
        async function handleFileUpload(type) {
            const fileInput = document.getElementById(type === 'cv' ? 'cv-upload' : 'job-upload');
            const statusEl = document.getElementById(type === 'cv' ? 'cv-status' : 'job-status');
            const file = fileInput.files[0];
            if (!file) return;

            // Validate file size (max 5MB)
            if (file.size > 5 * 1024 * 1024) {
                statusEl.textContent = '‚ùå File too large (max 5MB)';
                statusEl.style.color = '#dc3545';
                fileInput.value = '';
                return;
            }

            statusEl.textContent = '‚è≥ Uploading...';
            statusEl.style.color = '#856404';

            const formData = new FormData();
            formData.append('file', file);
            formData.append('type', type);

            try {
                const res = await fetch('/api/upload-document', {
                    method: 'POST',
                    body: formData,
                });
                const data = await res.json();
                if (res.ok && data.text) {
                    if (type === 'cv') uploadedCV = data.text;
                    else uploadedJobProfile = data.text;
                    statusEl.innerHTML = '‚úÖ ' + file.name + ' <span class="file-remove" onclick="removeUpload(\'' + type + '\')">‚úï remove</span>';
                    statusEl.style.color = '#28a745';
                } else {
                    statusEl.textContent = '‚ùå ' + (data.error || 'Upload failed');
                    statusEl.style.color = '#dc3545';
                    fileInput.value = '';
                }
            } catch(e) {
                statusEl.textContent = '‚ùå Upload error';
                statusEl.style.color = '#dc3545';
                fileInput.value = '';
            }
        }

        function removeUpload(type) {
            if (type === 'cv') {
                uploadedCV = '';
                document.getElementById('cv-upload').value = '';
                document.getElementById('cv-status').textContent = '';
            } else {
                uploadedJobProfile = '';
                document.getElementById('job-upload').value = '';
                document.getElementById('job-status').textContent = '';
            }
        }

        // ============================================================
        // VIDEO AVATAR: Show animated interviewer/agent
        // ============================================================
        function showVideoAvatar(mode) {
            const container = document.getElementById('video-avatar');
            const video = document.getElementById('avatar-video');
            const fallback = document.getElementById('avatar-fallback');
            const nameEl = document.getElementById('avatar-name');
            const role = ROLE_LABELS[mode] || ROLE_LABELS.interview;
            nameEl.textContent = role.name;
            fallback.className = 'fas ' + role.icon + ' avatar-fallback';

            container.style.display = 'block';

            // Try to load video for interview mode
            if (mode === 'interview') {
                video.play().then(() => {
                    video.style.display = 'block';
                    fallback.style.display = 'none';
                }).catch(() => {
                    // Video not available ‚Äî show icon fallback
                    video.style.display = 'none';
                    fallback.style.display = 'block';
                });
            } else {
                video.style.display = 'none';
                fallback.style.display = 'block';
            }
        }

        function hideVideoAvatar() {
            document.getElementById('video-avatar').style.display = 'none';
            const video = document.getElementById('avatar-video');
            video.pause();
            video.style.display = 'none';
            document.getElementById('avatar-fallback').style.display = 'block';
            setAvatarSpeaking(false);
        }

        function setAvatarSpeaking(isSpeaking) {
            const ring = document.getElementById('speaking-ring');
            if (isSpeaking) ring.classList.add('active');
            else ring.classList.remove('active');
        }

        // ============================================================
        // LIVE TRANSCRIPT: Show text for both candidate and bot
        // ============================================================
        function showTranscriptPanel() {
            document.getElementById('transcript-panel').style.display = 'block';
        }

        function hideTranscriptPanel() {
            document.getElementById('transcript-panel').style.display = 'none';
            document.getElementById('transcript-content').innerHTML = '';
        }

        function addTranscriptLine(role, text) {
            const panel = document.getElementById('transcript-panel');
            const content = document.getElementById('transcript-content');
            panel.style.display = 'block';

            const roleName = role === 'user' ? 'You' : (ROLE_LABELS[currentMode] || ROLE_LABELS.interview).name.split(' ‚Äî ')[0];
            const cssClass = role === 'user' ? 'user-line' : 'bot-line';
            const line = document.createElement('div');
            line.className = 'transcript-line ' + cssClass;
            line.innerHTML = '<span class=\"transcript-label\">' + escapeHTML(roleName) + ':</span> ' + escapeHTML(text);
            content.appendChild(line);

            // Keep only last 20 lines
            while (content.children.length > 20) content.removeChild(content.firstChild);
            panel.scrollTop = panel.scrollHeight;
        }

        function updateTranscriptLast(text) {
            const content = document.getElementById('transcript-content');
            if (content.lastChild) {
                const label = content.lastChild.querySelector('.transcript-label');
                const labelText = label ? label.outerHTML : '';
                content.lastChild.innerHTML = labelText + ' ' + escapeHTML(text);
            }
            const panel = document.getElementById('transcript-panel');
            panel.scrollTop = panel.scrollHeight;
        }

        // --- Active mode highlight ---
        function setActiveMode(modeId) {
            // Remove active from all mode buttons
            document.querySelectorAll('.mode-btn').forEach(btn => btn.classList.remove('active'));
            // Add active to selected
            if (modeId) {
                const btn = document.getElementById(modeId);
                if (btn) btn.classList.add('active');
            }
        }

        // --- Mode selection ---
        // Cleanup before switching modes ‚Äî cancel streams, stop audio, stop recording
        function cleanupBeforeModeSwitch() {
            // Disconnect Realtime API if active
            disconnectRealtime();
            // Legacy cleanup
            socket.emit('cancel_stream');
            stopCurrentAudio();
            flushAudioQueue();
            stopVAD();
            stopBrowserSTT();
            stopInterruptionMonitor();
            if (isRecording && mediaRecorder) {
                try { mediaRecorder.stop(); } catch(e) {}
                isRecording = false;
                micBtn.classList.remove('recording');
            }
            releaseMicStream();
            wasInterrupted = false;
            waitingForBotAudio = false;
            isProcessing = false;
        }

        function startInterview() {
            // Show the interview setup panel (CV/Job upload)
            currentMode = 'interview';
            setActiveMode('btn-interview');
            document.getElementById('interview-setup').style.display = 'block';
            document.getElementById('language-picker').style.display = 'none';
            statusEl.textContent = 'Upload your CV/Job Profile (optional) or start directly';
        }

        function startInterviewWithContext() {
            document.getElementById('interview-setup').style.display = 'none';
            cleanupBeforeModeSwitch();
            chatArea.innerHTML = '';
            statusEl.textContent = 'Interview Assistant';
            currentMode = 'interview';
            setActiveMode('btn-interview');
            startSessionTimer();
            showVideoAvatar('interview');
            showTranscriptPanel();

            if (voiceModeOn) {
                // REALTIME API: Direct browser ‚Üî OpenAI WebRTC
                // Pass CV/Job context via query params so backend can inject into prompt
                connectRealtime('interview', 'en', uploadedCV, uploadedJobProfile);
            } else {
                // Text mode: Socket.IO
                socket.emit('toggle_voice_mode', { voice_mode: false });
                showThinking();
                socket.emit('start_interview', { cv_text: uploadedCV, job_profile_text: uploadedJobProfile });
            }
        }

        function startHelpdesk() {
            cleanupBeforeModeSwitch();
            document.getElementById('interview-setup').style.display = 'none';
            chatArea.innerHTML = '';
            statusEl.textContent = 'IT Helpdesk';
            currentMode = 'helpdesk';
            setActiveMode('btn-helpdesk');
            startSessionTimer();
            showVideoAvatar('helpdesk');
            showTranscriptPanel();

            if (voiceModeOn) {
                connectRealtime('helpdesk');
            } else {
                socket.emit('toggle_voice_mode', { voice_mode: false });
                showThinking();
                socket.emit('start_helpdesk');
            }
        }

        function showLanguagePicker() {
            currentMode = 'language';
            setActiveMode('btn-language');
            document.getElementById('language-picker').style.display = 'block';
            document.getElementById('interview-setup').style.display = 'none';
        }

        function startLanguageTest() {
            cleanupBeforeModeSwitch();
            const lang = document.getElementById('lang-select').value;
            currentLanguage = lang;
            document.getElementById('language-picker').style.display = 'none';
            chatArea.innerHTML = '';
            statusEl.textContent = 'Language Practice Companion';
            startSessionTimer();
            showVideoAvatar('language');
            showTranscriptPanel();

            if (voiceModeOn) {
                connectRealtime('language', lang);
            } else {
                socket.emit('toggle_voice_mode', { voice_mode: false });
                showThinking();
                socket.emit('start_language_test', { language: lang });
            }
        }

        function resetSession() {
            // Disconnect Realtime API
            disconnectRealtime();
            // Stop all audio, streaming, recording, and browser STT
            socket.emit('cancel_stream');
            stopCurrentAudio();
            flushAudioQueue();
            stopVAD();
            stopBrowserSTT();
            stopInterruptionMonitor();
            if (typeof stopTTSPlayback === 'function') stopTTSPlayback();
            if (isRecording && mediaRecorder) {
                try { mediaRecorder.stop(); } catch(e) {}
                isRecording = false;
                micBtn.classList.remove('recording');
            }
            releaseMicStream();
            // Reset audio/recording state
            autoListenEnabled = true;
            wasInterrupted = false;
            speechDetected = false;
            silenceStart = null;
            voiceModeOn = true;
            document.getElementById('voice-toggle').checked = true;
            document.getElementById('voice-mode-label').textContent = 'Voice mode';
            document.getElementById('cost-badge').textContent = 'realtime AI';
            document.getElementById('cost-badge').className = 'cost-badge active';
            if (sessionTimerInterval) clearInterval(sessionTimerInterval);
            document.getElementById('session-timer').textContent = '00:00';
            pendingTtsMessages = {};
            chatArea.innerHTML = '';
            document.getElementById('language-picker').style.display = 'none';
            document.getElementById('interview-setup').style.display = 'none';
            hideVideoAvatar();
            hideTranscriptPanel();
            // Send reset with current mode so backend re-initializes correctly
            socket.emit('reset', { mode: currentMode, language: currentLanguage });

            // Stay in the same module ‚Äî keep button highlighted, show ready status
            if (currentMode === 'interview') {
                setActiveMode('btn-interview');
                statusEl.textContent = 'Interview Assistant ‚Äî tap mic or type to start';
            } else if (currentMode === 'helpdesk') {
                setActiveMode('btn-helpdesk');
                statusEl.textContent = 'IT Helpdesk ‚Äî tap mic or type to start';
            } else if (currentMode === 'language') {
                setActiveMode('btn-language');
                statusEl.textContent = 'Language Practice ‚Äî tap mic or type to start';
            } else {
                setActiveMode(null);
                statusEl.textContent = 'Choose a mode to start';
            }
        }

        // --- Text input ---
        function sendText() {
            if (!isConnected && !realtimeConnected) {
                statusEl.textContent = 'Not connected ‚Äî please wait...';
                return;
            }
            let text = textInput.value.trim();
            if (!text) return;
            // SECURITY: Client-side length limit
            if (text.length > 2000) {
                text = text.substring(0, 2000);
                statusEl.textContent = 'Message trimmed to 2000 characters.';
            }

            if (realtimeConnected && realtimeDc) {
                // REALTIME MODE: Send text through WebRTC data channel
                addUserMessage(text);
                textInput.value = '';
                // Create a conversation item with the text
                realtimeDc.send(JSON.stringify({
                    type: 'conversation.item.create',
                    item: {
                        type: 'message',
                        role: 'user',
                        content: [{ type: 'input_text', text: text }],
                    },
                }));
                // Trigger response generation
                realtimeDc.send(JSON.stringify({
                    type: 'response.create',
                    response: { modalities: ['audio', 'text'] },
                }));
                // Log to backend
                socket.emit('realtime_log', { role: 'user', text: text, mode: currentMode });
            } else {
                // LEGACY MODE: Socket.IO text
                socket.emit('cancel_stream');
                stopCurrentAudio();
                addUserMessage(text);
                textInput.value = '';
                showThinking();
                socket.emit('text_message', { text: text, mode: currentMode });
            }
        }

        // --- Audio recording with Voice Activity Detection ---
        async function startListening(existingStream) {
            // Don't run legacy recording when Realtime API handles everything
            if (realtimeConnected) return;
            if (isRecording) return;
            // If bot is still playing, don't start recording (use interrupt monitor instead)
            if (botIsPlaying) return;
            // Stop any lingering interrupt monitor
            stopInterruptionMonitor();

            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                statusEl.textContent = 'Browser does not support audio recording';
                return;
            }
            try {
                // Reuse pre-warmed or existing mic stream (avoids 100-300ms getUserMedia delay)
                if (existingStream && existingStream.active) {
                    micStream = existingStream;
                    console.log('[Mic] Reusing mic stream (zero acquisition delay)');
                } else {
                    // Release dead stream if any before acquiring new one
                    if (micStream) { micStream.getTracks().forEach(t => t.stop()); micStream = null; }
                    micStream = await navigator.mediaDevices.getUserMedia({
                        audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: true }
                    });
                }

                // Close old AudioContext to prevent resource leaks
                if (audioContext) {
                    audioContext.close().catch(() => {});
                    audioContext = null;
                    analyser = null;
                }

                // Set up Web Audio API for VAD
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioContext.createMediaStreamSource(micStream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 2048; // Reliable RMS ‚Äî 42ms window (512 was only 10ms, too noisy)
                source.connect(analyser);

                // Set up MediaRecorder
                const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
                    ? 'audio/webm;codecs=opus' : 'audio/webm';
                mediaRecorder = new MediaRecorder(micStream, { mimeType });
                audioChunks = [];

                mediaRecorder.ondataavailable = e => {
                    if (e.data.size > 0) audioChunks.push(e.data);
                };

                mediaRecorder.onstop = () => {
                    stopVAD();

                    // ============================================================
                    // FAST PATH: Use browser SpeechRecognition text (instant)
                    // SLOW PATH: Use Whisper via audio upload (fallback)
                    // ============================================================
                    const browserText = getBrowserSTTText();
                    stopBrowserSTT();

                    // If still processing a previous response, cancel it first
                    if (isProcessing) {
                        socket.emit('cancel_stream');
                        stopCurrentAudio();
                        isProcessing = false;
                        wasInterrupted = true;
                    }

                    if (browserText && browserText.length > 1) {
                        // FAST PATH: Browser already transcribed ‚Äî send as text instantly
                        console.log('[STT] Using browser transcription:', browserText.substring(0, 50));
                        addUserMessage(browserText);
                        showThinking();
                        socket.emit('text_message', { text: browserText, interrupted: wasInterrupted, mode: currentMode });
                        wasInterrupted = false;
                        // Release mic ‚Äî bot will respond with interrupt monitor
                        releaseMicStream();
                    } else {
                        // SLOW PATH: No browser STT text ‚Äî use Whisper (upload audio)
                        const blob = new Blob(audioChunks, { type: mimeType });
                        if (speechDetected && blob.size > 2000) {
                            const reader = new FileReader();
                            reader.onload = () => {
                                const b64 = btoa(
                                    new Uint8Array(reader.result)
                                        .reduce((data, byte) => data + String.fromCharCode(byte), '')
                                );
                                // Don't show user message here ‚Äî backend will emit transcription
                                showThinking();
                                socket.emit('audio_message', { audio: b64, mimeType: mimeType, interrupted: wasInterrupted, mode: currentMode });
                                wasInterrupted = false;
                            };
                            reader.readAsArrayBuffer(blob);
                            // Release mic ‚Äî bot will respond with interrupt monitor
                            releaseMicStream();
                        } else {
                            // NO SPEECH ‚Äî reuse mic stream for instant restart (no getUserMedia gap)
                            if (autoListenEnabled && !botIsPlaying) {
                                statusEl.textContent = 'üé§ Waiting for you to speak...';
                                // Pass existing mic ‚Äî avoids 100-300ms getUserMedia delay
                                startListening(micStream);
                                return; // Don't release mic ‚Äî startListening reuses it
                            }
                            if (!autoListenEnabled) {
                                statusEl.textContent = 'No speech detected. Tap mic to try again.';
                            }
                            releaseMicStream();
                        }
                    }
                };

                mediaRecorder.start(250);
                isRecording = true;
                autoListenEnabled = true;
                micBtn.classList.add('recording');
                statusEl.textContent = 'üé§ Listening... (speak, I\'ll detect when you stop)';

                // Start browser SpeechRecognition for instant transcription (English only)
                if (useBrowserSTT && currentMode !== 'language') {
                    startBrowserSTT();
                }

                // Start voice activity detection
                startVAD();

            } catch (err) {
                if (err.name === 'NotFoundError') {
                    statusEl.textContent = 'No microphone found. Please connect one.';
                } else if (err.name === 'NotAllowedError') {
                    statusEl.textContent = 'Microphone permission denied. Allow it in browser settings.';
                } else {
                    statusEl.textContent = 'Mic error: ' + err.message;
                }
            }
        }

        function stopRecordingAndSend() {
            if (!isRecording || !mediaRecorder) return;
            mediaRecorder.stop();
            isRecording = false;
            micBtn.classList.remove('recording');
            // If browser STT is active, show instant status (no "processing" delay)
            if (useBrowserSTT && browserSTTResult) {
                statusEl.textContent = '‚ö° Sending...';
            } else {
                statusEl.textContent = 'Processing audio...';
            }
        }

        // Toggle: click mic to start/stop voice, or mute/unmute in Realtime mode
        async function toggleRecording() {
            if (realtimeConnected) {
                // REALTIME MODE: Mute/unmute mic
                toggleRealtimeMute();
                return;
            }
            // LEGACY MODE: Start/stop recording with VAD
            if (!isRecording) {
                await startListening();
            } else {
                // Manual stop (force-stop override)
                stopRecordingAndSend();
            }
        }

        // --- PWA: Register Service Worker ---
        if ('serviceWorker' in navigator) {
            window.addEventListener('load', () => {
                navigator.serviceWorker.register('/sw.js', { scope: '/' })
                    .then((reg) => console.log('[PWA] Service Worker registered, scope:', reg.scope))
                    .catch((err) => console.log('[PWA] SW registration failed:', err));
            });
        }
    </script>
    <script src="/static/voice.js"></script>
</body>
</html>